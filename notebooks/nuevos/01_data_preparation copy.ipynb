{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de Datos con PySpark\n",
    "\n",
    "Este notebook realiza la preparación de datos usando PySpark para procesar documentos PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando Spark...\n",
      "Spark configurado correctamente.\n",
      "Directorio data/raw ya existe.\n",
      "Directorio data/processed ya existe.\n",
      "Directorios configurados: RAW_DIR=data/raw, PROCESSED_DIR=data/processed\n"
     ]
    }
   ],
   "source": [
    "# Configuración inicial\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Instalar pdf2image si no está disponible\n",
    "try:\n",
    "    import pdf2image\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pdf2image\"])\n",
    "finally:\n",
    "    from pdf2image import convert_from_path\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Configurar Spark\n",
    "print(\"Configurando Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFPreprocessing\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark configurado correctamente.\")\n",
    "\n",
    "# Configurar directorios\n",
    "RAW_DIR = 'data/raw'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "\n",
    "# Validar y crear directorios si no existen\n",
    "if not os.path.exists(RAW_DIR):\n",
    "    print(f\"Directorio {RAW_DIR} no existe. Creándolo...\")\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directorio {RAW_DIR} ya existe.\")\n",
    "\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    print(f\"Directorio {PROCESSED_DIR} no existe. Creándolo...\")\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directorio {PROCESSED_DIR} ya existe.\")\n",
    "\n",
    "print(f\"Directorios configurados: RAW_DIR={RAW_DIR}, PROCESSED_DIR={PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando data/raw\\2-TITULOS-15-DE-NOVIEMBRE-2024.pdf...\n",
      "Guardada página 0 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_0.png\n",
      "Guardada página 1 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_1.png\n",
      "Guardada página 2 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_2.png\n",
      "Guardada página 3 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_3.png\n",
      "Guardada página 4 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_4.png\n",
      "Guardada página 5 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_5.png\n",
      "Guardada página 6 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_6.png\n",
      "Guardada página 7 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_7.png\n",
      "Guardada página 8 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_8.png\n",
      "Guardada página 9 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_9.png\n",
      "Guardada página 10 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_10.png\n",
      "Guardada página 11 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_11.png\n",
      "Guardada página 12 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_12.png\n",
      "Guardada página 13 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_13.png\n",
      "Guardada página 14 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_14.png\n",
      "Guardada página 15 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_15.png\n",
      "Guardada página 16 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_16.png\n",
      "Guardada página 17 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_17.png\n",
      "Guardada página 18 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_18.png\n",
      "Guardada página 19 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_19.png\n",
      "Guardada página 20 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_20.png\n",
      "Guardada página 21 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_21.png\n",
      "Guardada página 22 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_22.png\n",
      "Guardada página 23 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_23.png\n",
      "Guardada página 24 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_24.png\n",
      "Guardada página 25 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_25.png\n",
      "Guardada página 26 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_26.png\n",
      "Guardada página 27 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_27.png\n",
      "Guardada página 28 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_28.png\n",
      "Guardada página 29 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_29.png\n",
      "Guardada página 30 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_30.png\n",
      "Guardada página 31 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_31.png\n",
      "Guardada página 32 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_32.png\n",
      "Guardada página 33 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_33.png\n",
      "Guardada página 34 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_34.png\n",
      "Guardada página 35 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_35.png\n",
      "Guardada página 36 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_36.png\n",
      "Guardada página 37 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_37.png\n",
      "Guardada página 38 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_38.png\n",
      "Guardada página 39 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_39.png\n",
      "Guardada página 40 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_40.png\n",
      "Guardada página 41 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_41.png\n",
      "Guardada página 42 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_42.png\n",
      "Guardada página 43 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_43.png\n",
      "Guardada página 44 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_44.png\n",
      "Guardada página 45 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_45.png\n",
      "Guardada página 46 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_46.png\n",
      "Guardada página 47 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_47.png\n",
      "Guardada página 48 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_48.png\n",
      "Guardada página 49 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_49.png\n",
      "Guardada página 50 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_50.png\n",
      "Guardada página 51 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_51.png\n",
      "Guardada página 52 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_52.png\n",
      "Guardada página 53 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_53.png\n",
      "Guardada página 54 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_54.png\n",
      "Guardada página 55 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_55.png\n",
      "Guardada página 56 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_56.png\n",
      "Guardada página 57 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_57.png\n",
      "Guardada página 58 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_58.png\n",
      "Guardada página 59 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_59.png\n",
      "Guardada página 60 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_60.png\n",
      "Guardada página 61 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_61.png\n",
      "Guardada página 62 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_62.png\n",
      "Guardada página 63 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_63.png\n",
      "Guardada página 64 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_64.png\n",
      "Guardada página 65 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_65.png\n",
      "Guardada página 66 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_66.png\n",
      "Guardada página 67 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_67.png\n",
      "Guardada página 68 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_68.png\n",
      "Guardada página 69 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_69.png\n",
      "Guardada página 70 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_70.png\n",
      "Guardada página 71 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_71.png\n",
      "Guardada página 72 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_72.png\n",
      "Guardada página 73 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_73.png\n",
      "Guardada página 74 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_74.png\n",
      "Guardada página 75 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_75.png\n",
      "Guardada página 76 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_76.png\n",
      "Guardada página 77 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_77.png\n",
      "Guardada página 78 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_78.png\n",
      "Guardada página 79 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_79.png\n",
      "Guardada página 80 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_80.png\n",
      "Guardada página 81 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_81.png\n",
      "Guardada página 82 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_82.png\n",
      "Guardada página 83 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_83.png\n",
      "Guardada página 84 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_84.png\n",
      "Guardada página 85 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_85.png\n",
      "Guardada página 86 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_86.png\n",
      "Guardada página 87 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_87.png\n",
      "Guardada página 88 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_88.png\n",
      "Guardada página 89 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_89.png\n",
      "Guardada página 90 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_90.png\n",
      "Guardada página 91 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_91.png\n",
      "Guardada página 92 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_92.png\n",
      "Guardada página 93 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_93.png\n",
      "Guardada página 94 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_94.png\n",
      "Guardada página 95 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_95.png\n",
      "Guardada página 96 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_96.png\n",
      "Guardada página 97 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_97.png\n",
      "Guardada página 98 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_98.png\n",
      "Guardada página 99 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_99.png\n",
      "Guardada página 100 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_100.png\n",
      "Guardada página 101 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_101.png\n",
      "Guardada página 102 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_102.png\n",
      "Guardada página 103 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_103.png\n",
      "Guardada página 104 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_104.png\n",
      "Guardada página 105 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_105.png\n",
      "Guardada página 106 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_106.png\n",
      "Guardada página 107 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_107.png\n",
      "Guardada página 108 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_108.png\n",
      "Guardada página 109 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_109.png\n",
      "Guardada página 110 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_110.png\n",
      "Guardada página 111 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_111.png\n",
      "Guardada página 112 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_112.png\n",
      "Guardada página 113 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_113.png\n",
      "Guardada página 114 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_114.png\n",
      "Guardada página 115 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_115.png\n",
      "Guardada página 116 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_116.png\n",
      "Guardada página 117 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_117.png\n",
      "Guardada página 118 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_118.png\n",
      "Guardada página 119 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_119.png\n",
      "Guardada página 120 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_120.png\n",
      "Guardada página 121 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_121.png\n",
      "Guardada página 122 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_122.png\n",
      "Guardada página 123 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_123.png\n",
      "Guardada página 124 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_124.png\n",
      "Guardada página 125 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_125.png\n",
      "Guardada página 126 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_126.png\n",
      "Guardada página 127 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_127.png\n",
      "Guardada página 128 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_128.png\n",
      "Guardada página 129 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_129.png\n",
      "Guardada página 130 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_130.png\n",
      "Guardada página 131 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_131.png\n",
      "Guardada página 132 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_132.png\n",
      "Guardada página 133 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_133.png\n",
      "Guardada página 134 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_134.png\n",
      "Guardada página 135 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_135.png\n",
      "Guardada página 136 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_136.png\n",
      "Guardada página 137 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_137.png\n",
      "Guardada página 138 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_138.png\n",
      "Guardada página 139 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_139.png\n",
      "Guardada página 140 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_140.png\n",
      "Guardada página 141 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_141.png\n",
      "Guardada página 142 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_142.png\n",
      "Guardada página 143 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_143.png\n",
      "Guardada página 144 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_144.png\n",
      "Guardada página 145 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_145.png\n",
      "Guardada página 146 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_146.png\n",
      "Guardada página 147 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_147.png\n",
      "Guardada página 148 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_148.png\n",
      "Guardada página 149 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_149.png\n",
      "Guardada página 150 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_150.png\n",
      "Guardada página 151 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_151.png\n",
      "Guardada página 152 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_152.png\n",
      "Guardada página 153 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_153.png\n",
      "Guardada página 154 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_154.png\n",
      "Guardada página 155 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_155.png\n",
      "Guardada página 156 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_156.png\n",
      "Guardada página 157 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_157.png\n",
      "Guardada página 158 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_158.png\n",
      "Guardada página 159 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_159.png\n",
      "Guardada página 160 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_160.png\n",
      "Guardada página 161 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_161.png\n",
      "Guardada página 162 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_162.png\n",
      "Guardada página 163 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_163.png\n",
      "Guardada página 164 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_164.png\n",
      "Guardada página 165 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_165.png\n",
      "Guardada página 166 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_166.png\n",
      "Guardada página 167 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_167.png\n",
      "Guardada página 168 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_168.png\n",
      "Guardada página 169 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_169.png\n",
      "Guardada página 170 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_170.png\n",
      "Guardada página 171 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_171.png\n",
      "Guardada página 172 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_172.png\n",
      "Guardada página 173 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_173.png\n",
      "Guardada página 174 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_174.png\n",
      "Guardada página 175 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_175.png\n",
      "Guardada página 176 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_176.png\n",
      "Guardada página 177 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_177.png\n",
      "Guardada página 178 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_178.png\n",
      "Guardada página 179 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_179.png\n",
      "Guardada página 180 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_180.png\n",
      "Guardada página 181 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_181.png\n",
      "Guardada página 182 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_182.png\n",
      "Guardada página 183 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_183.png\n",
      "Guardada página 184 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_184.png\n",
      "Guardada página 185 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_185.png\n",
      "Guardada página 186 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_186.png\n",
      "Guardada página 187 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_187.png\n",
      "Guardada página 188 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_188.png\n",
      "Guardada página 189 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_189.png\n",
      "Guardada página 190 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_190.png\n",
      "Guardada página 191 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_191.png\n",
      "Guardada página 192 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_192.png\n",
      "Guardada página 193 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_193.png\n",
      "Guardada página 194 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_194.png\n",
      "Guardada página 195 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_195.png\n",
      "Guardada página 196 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_196.png\n",
      "Guardada página 197 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_197.png\n",
      "Guardada página 198 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_198.png\n",
      "Guardada página 199 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_199.png\n",
      "Guardada página 200 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_200.png\n",
      "Guardada página 201 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_201.png\n",
      "Guardada página 202 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_202.png\n",
      "Guardada página 203 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_203.png\n",
      "Guardada página 204 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_204.png\n",
      "Guardada página 205 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_205.png\n",
      "Guardada página 206 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_206.png\n",
      "Guardada página 207 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_207.png\n",
      "Guardada página 208 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_208.png\n",
      "Guardada página 209 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_209.png\n",
      "Guardada página 210 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_210.png\n",
      "Guardada página 211 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_211.png\n",
      "Guardada página 212 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_212.png\n",
      "Guardada página 213 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_213.png\n",
      "Guardada página 214 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_214.png\n",
      "Guardada página 215 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_215.png\n",
      "Guardada página 216 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_216.png\n",
      "Guardada página 217 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_217.png\n",
      "Guardada página 218 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_218.png\n",
      "Guardada página 219 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_219.png\n",
      "Guardada página 220 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_220.png\n",
      "Guardada página 221 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_221.png\n",
      "Guardada página 222 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_222.png\n",
      "Guardada página 223 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_223.png\n",
      "Guardada página 224 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_224.png\n",
      "Guardada página 225 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_225.png\n",
      "Guardada página 226 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_226.png\n",
      "Guardada página 227 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_227.png\n",
      "Guardada página 228 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_228.png\n",
      "Guardada página 229 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_229.png\n",
      "Guardada página 230 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_230.png\n",
      "Guardada página 231 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_231.png\n",
      "Guardada página 232 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_232.png\n",
      "Guardada página 233 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_233.png\n",
      "Guardada página 234 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_234.png\n",
      "Guardada página 235 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_235.png\n",
      "Guardada página 236 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_236.png\n",
      "Guardada página 237 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_237.png\n",
      "Guardada página 238 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_238.png\n",
      "Guardada página 239 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_239.png\n",
      "Guardada página 240 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_240.png\n",
      "Guardada página 241 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_241.png\n",
      "Guardada página 242 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_242.png\n",
      "Guardada página 243 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_243.png\n",
      "Guardada página 244 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_244.png\n",
      "Guardada página 245 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_245.png\n",
      "Guardada página 246 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_246.png\n",
      "Guardada página 247 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_247.png\n",
      "Guardada página 248 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_248.png\n",
      "Guardada página 249 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_249.png\n",
      "Guardada página 250 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_250.png\n",
      "Guardada página 251 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_251.png\n",
      "Guardada página 252 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_252.png\n",
      "Guardada página 253 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_253.png\n",
      "Guardada página 254 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_254.png\n",
      "Guardada página 255 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_255.png\n",
      "Guardada página 256 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_256.png\n",
      "Guardada página 257 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_257.png\n",
      "Guardada página 258 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_258.png\n",
      "Guardada página 259 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_259.png\n",
      "Guardada página 260 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_260.png\n",
      "Guardada página 261 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_261.png\n",
      "Guardada página 262 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_262.png\n",
      "Guardada página 263 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_263.png\n",
      "Guardada página 264 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_264.png\n",
      "Guardada página 265 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_265.png\n",
      "Guardada página 266 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_266.png\n",
      "Guardada página 267 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_267.png\n",
      "Guardada página 268 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_268.png\n",
      "Guardada página 269 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_269.png\n",
      "Guardada página 270 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_270.png\n",
      "Guardada página 271 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_271.png\n",
      "Guardada página 272 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_272.png\n",
      "Guardada página 273 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_273.png\n",
      "Guardada página 274 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_274.png\n",
      "Guardada página 275 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_275.png\n",
      "Guardada página 276 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_276.png\n",
      "Guardada página 277 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_277.png\n",
      "Guardada página 278 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_278.png\n",
      "Guardada página 279 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_279.png\n",
      "Guardada página 280 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_280.png\n",
      "Guardada página 281 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_281.png\n",
      "Guardada página 282 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_282.png\n",
      "Guardada página 283 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_283.png\n",
      "Guardada página 284 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_284.png\n",
      "Guardada página 285 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_285.png\n",
      "Guardada página 286 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_286.png\n",
      "Guardada página 287 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_287.png\n",
      "Guardada página 288 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_288.png\n",
      "Guardada página 289 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_289.png\n",
      "Guardada página 290 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_290.png\n",
      "Guardada página 291 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_291.png\n",
      "Guardada página 292 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_292.png\n",
      "Guardada página 293 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_293.png\n",
      "Guardada página 294 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_294.png\n",
      "Guardada página 295 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_295.png\n",
      "Guardada página 296 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_296.png\n",
      "Guardada página 297 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_297.png\n",
      "Guardada página 298 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_298.png\n",
      "Guardada página 299 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_299.png\n",
      "Guardada página 300 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_300.png\n",
      "Guardada página 301 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_301.png\n",
      "Guardada página 302 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_302.png\n",
      "Guardada página 303 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_303.png\n",
      "Guardada página 304 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_304.png\n",
      "Guardada página 305 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_305.png\n",
      "Guardada página 306 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_306.png\n",
      "Guardada página 307 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_307.png\n",
      "Guardada página 308 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_308.png\n",
      "Guardada página 309 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_309.png\n",
      "Guardada página 310 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_310.png\n",
      "Guardada página 311 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_311.png\n",
      "Guardada página 312 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_312.png\n",
      "Guardada página 313 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_313.png\n",
      "Guardada página 314 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_314.png\n",
      "Guardada página 315 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_315.png\n",
      "Guardada página 316 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_316.png\n",
      "Guardada página 317 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_317.png\n",
      "Guardada página 318 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_318.png\n",
      "Guardada página 319 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_319.png\n",
      "Guardada página 320 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_320.png\n",
      "Guardada página 321 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_321.png\n",
      "Guardada página 322 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_322.png\n",
      "Guardada página 323 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_323.png\n",
      "Guardada página 324 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_324.png\n",
      "Guardada página 325 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_325.png\n",
      "Guardada página 326 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_326.png\n",
      "Guardada página 327 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_327.png\n",
      "Guardada página 328 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_328.png\n",
      "Guardada página 329 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_329.png\n",
      "Guardada página 330 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_330.png\n",
      "Guardada página 331 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_331.png\n",
      "Guardada página 332 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_332.png\n",
      "Guardada página 333 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_333.png\n",
      "Guardada página 334 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_334.png\n",
      "Guardada página 335 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_335.png\n",
      "Guardada página 336 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_336.png\n",
      "Guardada página 337 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_337.png\n",
      "Guardada página 338 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_338.png\n",
      "Guardada página 339 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_339.png\n",
      "Guardada página 340 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_340.png\n",
      "Guardada página 341 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_341.png\n",
      "Guardada página 342 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_342.png\n",
      "Guardada página 343 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_343.png\n",
      "Guardada página 344 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_344.png\n",
      "Guardada página 345 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_345.png\n",
      "Guardada página 346 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_346.png\n",
      "Guardada página 347 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_347.png\n",
      "Guardada página 348 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_348.png\n",
      "Guardada página 349 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_349.png\n",
      "Guardada página 350 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_350.png\n",
      "Guardada página 351 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_351.png\n",
      "Guardada página 352 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_352.png\n",
      "Guardada página 353 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_353.png\n",
      "Guardada página 354 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_354.png\n",
      "Guardada página 355 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_355.png\n",
      "Guardada página 356 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_356.png\n",
      "Guardada página 357 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_357.png\n",
      "Guardada página 358 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_358.png\n",
      "Guardada página 359 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_359.png\n",
      "Guardada página 360 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_360.png\n",
      "Guardada página 361 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_361.png\n",
      "Guardada página 362 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_362.png\n",
      "Guardada página 363 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_363.png\n",
      "Guardada página 364 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_364.png\n",
      "Guardada página 365 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_365.png\n",
      "Guardada página 366 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_366.png\n",
      "Guardada página 367 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_367.png\n",
      "Guardada página 368 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_368.png\n",
      "Guardada página 369 de 2-TITULOS-15-DE-NOVIEMBRE-2024 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024\\page_369.png\n",
      "Procesamiento de 2-TITULOS-15-DE-NOVIEMBRE-2024 completado.\n"
     ]
    }
   ],
   "source": [
    "# Procesar PDFs a imágenes\n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Procesa un PDF y guarda sus páginas como imágenes\"\"\"\n",
    "    try:\n",
    "        print(f\"Procesando {pdf_path}...\")\n",
    "        # Convertir PDF a imágenes\n",
    "        pages = convert_from_path(pdf_path)\n",
    "        pdf_name = os.path.basename(pdf_path).replace('.pdf', '')\n",
    "        \n",
    "        # Crear directorio para este PDF\n",
    "        pdf_dir = os.path.join(PROCESSED_DIR, pdf_name)\n",
    "        os.makedirs(pdf_dir, exist_ok=True)\n",
    "        \n",
    "        # Guardar cada página como imagen\n",
    "        for i, page in enumerate(pages):\n",
    "            page_path = os.path.join(pdf_dir, f'page_{i}.png')\n",
    "            page.save(page_path)\n",
    "            print(f\"Guardada página {i} de {pdf_name} en {page_path}\")\n",
    "            \n",
    "        print(f\"Procesamiento de {pdf_name} completado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {str(e)}\")\n",
    "\n",
    "# Procesar todos los PDFs en el directorio RAW_DIR\n",
    "pdf_files = [os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith('.pdf')]\n",
    "for pdf_file in pdf_files:\n",
    "    process_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Página 110 marcada como primera página\n",
      "INFO:__main__:Página 114 marcada como primera página\n",
      "INFO:__main__:Página 146 marcada como primera página\n",
      "INFO:__main__:Página 170 marcada como primera página\n",
      "INFO:__main__:Página 178 marcada como primera página\n",
      "INFO:__main__:Página 184 marcada como primera página\n",
      "INFO:__main__:Página 190 marcada como primera página\n",
      "INFO:__main__:Página 198 marcada como primera página\n",
      "INFO:__main__:Página 206 marcada como primera página\n",
      "INFO:__main__:Página 214 marcada como primera página\n",
      "INFO:__main__:Página 222 marcada como primera página\n",
      "INFO:__main__:Página 232 marcada como primera página\n",
      "INFO:__main__:Página 244 marcada como primera página\n",
      "INFO:__main__:Página 254 marcada como primera página\n",
      "INFO:__main__:Página 264 marcada como primera página\n",
      "INFO:__main__:Página 274 marcada como primera página\n",
      "INFO:__main__:Página 284 marcada como primera página\n",
      "INFO:__main__:Página 290 marcada como primera página\n",
      "INFO:__main__:Página 296 marcada como primera página\n",
      "INFO:__main__:Página 312 marcada como primera página\n",
      "INFO:__main__:Página 322 marcada como primera página\n",
      "INFO:__main__:Página 332 marcada como primera página\n",
      "INFO:__main__:Página 346 marcada como primera página\n",
      "INFO:__main__:Página 356 marcada como primera página\n",
      "INFO:__main__:Página 360 marcada como primera página\n",
      "INFO:__main__:Etiquetas guardadas correctamente\n",
      "INFO:__main__:Todos los PDFs han sido etiquetados\n",
      "INFO:__main__:Etiquetas guardadas correctamente\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFLabeler:\n",
    "    def __init__(self, processed_dir='data/processed', labels_file='data/labels.json'):\n",
    "        self.processed_dir = processed_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"PDF Page Labeler\")\n",
    "        self.setup_gui()\n",
    "        self.load_existing_labels()\n",
    "\n",
    "    def setup_gui(self):\n",
    "        \"\"\"Configura la interfaz gráfica\"\"\"\n",
    "        # Frame principal\n",
    "        self.main_frame = ttk.Frame(self.window, padding=\"10\")\n",
    "        self.main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "\n",
    "        # Área de visualización\n",
    "        self.image_label = ttk.Label(self.main_frame)\n",
    "        self.image_label.grid(row=0, column=0, rowspan=6, pady=5)\n",
    "\n",
    "        # Controles\n",
    "        ttk.Label(self.main_frame, text=\"PDF actual:\").grid(\n",
    "            row=0, column=1, sticky=tk.W)\n",
    "        self.pdf_label = ttk.Label(self.main_frame, text=\"\")\n",
    "        self.pdf_label.grid(row=0, column=2, sticky=tk.W)\n",
    "\n",
    "        ttk.Label(self.main_frame, text=\"Página:\").grid(\n",
    "            row=1, column=1, sticky=tk.W)\n",
    "        self.page_label = ttk.Label(self.main_frame, text=\"\")\n",
    "        self.page_label.grid(row=1, column=2, sticky=tk.W)\n",
    "\n",
    "        # Botones\n",
    "        ttk.Button(self.main_frame, text=\"Es Primera Página\",\n",
    "                   command=self.mark_first_page).grid(row=2, column=1, pady=5)\n",
    "        ttk.Button(self.main_frame, text=\"No es Primera Página\",\n",
    "                   command=self.next_page).grid(row=2, column=2, pady=5)\n",
    "        ttk.Button(self.main_frame, text=\"Guardar y Salir\",\n",
    "                   command=self.save_and_exit).grid(row=3, column=1, columnspan=2, pady=5)\n",
    "        \n",
    "        # Agregar botones de navegación\n",
    "        ttk.Button(self.main_frame, text=\"← Anterior\",\n",
    "                   command=self.previous_page).grid(row=4, column=1, pady=5)\n",
    "        ttk.Button(self.main_frame, text=\"Siguiente →\",\n",
    "                   command=self.next_page).grid(row=4, column=2, pady=5)\n",
    "\n",
    "        # Mostrar estado de etiquetado\n",
    "        self.status_label = ttk.Label(self.main_frame, text=\"\")\n",
    "        self.status_label.grid(row=5, column=1, columnspan=2, pady=5)\n",
    "\n",
    "        # Agregar atajos de teclado\n",
    "        self.window.bind('<Left>', lambda e: self.previous_page())\n",
    "        self.window.bind('<Right>', lambda e: self.next_page())\n",
    "        self.window.bind('<space>', lambda e: self.mark_first_page())\n",
    "        self.window.bind('q', lambda e: self.save_and_exit())\n",
    "\n",
    "    def previous_page(self):\n",
    "        \"\"\"Retrocede a la página anterior\"\"\"\n",
    "        if self.current_page > 0:\n",
    "            self.current_page -= 1\n",
    "            self.show_current_page()\n",
    "\n",
    "    def show_current_page(self):\n",
    "        \"\"\"Muestra la página actual\"\"\"\n",
    "        try:\n",
    "            if self.current_pdf_index >= len(self.pdfs):\n",
    "                return  # No intentar mostrar una página si ya hemos terminado\n",
    "\n",
    "            page_path = os.path.join(\n",
    "                self.processed_dir,\n",
    "                self.current_pdf,\n",
    "                f'page_{self.current_page}.png'\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(page_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"No se encuentra la página: {page_path}\")\n",
    "\n",
    "            # Cargar y mostrar imagen\n",
    "            image = Image.open(page_path)\n",
    "\n",
    "            # Mantener proporción de aspecto\n",
    "            width, height = image.size\n",
    "            ratio = min(800/width, 600/height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "            photo = ImageTk.PhotoImage(image)\n",
    "            self.image_label.configure(image=photo)\n",
    "            self.image_label.image = photo\n",
    "\n",
    "            # Actualizar etiquetas\n",
    "            self.pdf_label.configure(text=self.current_pdf)\n",
    "            self.page_label.configure(\n",
    "                text=f\"Página {self.current_page + 1} de {self.get_total_pages()}\")\n",
    "\n",
    "            # Actualizar estado\n",
    "            is_marked = self.is_page_marked(self.current_page)\n",
    "            status = \"MARCADA COMO PRIMERA PÁGINA\" if is_marked else \"NO MARCADA\"\n",
    "            self.status_label.configure(text=f\"Estado: {status}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error mostrando página: {e}\")\n",
    "            self.status_label.configure(text=\"Error mostrando la página\")\n",
    "\n",
    "    def get_total_pages(self):\n",
    "        \"\"\"Obtiene el total de páginas del PDF actual\"\"\"\n",
    "        pdf_dir = os.path.join(self.processed_dir, self.current_pdf)\n",
    "        return len([f for f in os.listdir(pdf_dir) if f.endswith('.png')])\n",
    "\n",
    "    def is_page_marked(self, page_num):\n",
    "        \"\"\"Verifica si la página está marcada como primera página\"\"\"\n",
    "        return (self.current_pdf in self.labels and\n",
    "                page_num in self.labels[self.current_pdf][\"target_pages\"])\n",
    "\n",
    "    def load_existing_labels(self):\n",
    "        \"\"\"Carga etiquetas existentes si las hay\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.labels_file):\n",
    "                with open(self.labels_file, 'r') as f:\n",
    "                    self.labels = json.load(f)\n",
    "            else:\n",
    "                self.labels = {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cargando etiquetas: {e}\")\n",
    "            self.labels = {}\n",
    "\n",
    "    def start_labeling(self):\n",
    "        \"\"\"Inicia el proceso de etiquetado\"\"\"\n",
    "        pdfs = [d for d in os.listdir(self.processed_dir)\n",
    "                if os.path.isdir(os.path.join(self.processed_dir, d))]\n",
    "\n",
    "        if not pdfs:\n",
    "            logger.error(\"No se encontraron PDFs procesados\")\n",
    "            return\n",
    "\n",
    "        self.pdfs = pdfs\n",
    "        self.current_pdf_index = 0\n",
    "        self.current_pdf = self.pdfs[self.current_pdf_index]\n",
    "        self.current_page = 0\n",
    "        self.show_current_page()\n",
    "        self.window.mainloop()\n",
    "\n",
    "    def mark_first_page(self):\n",
    "        \"\"\"Marca la página actual como primera página\"\"\"\n",
    "        if self.current_pdf not in self.labels:\n",
    "            self.labels[self.current_pdf] = {\n",
    "                \"target_pages\": [], \"total_pages\": 0}\n",
    "\n",
    "        if self.current_page not in self.labels[self.current_pdf][\"target_pages\"]:\n",
    "            self.labels[self.current_pdf][\"target_pages\"].append(\n",
    "                self.current_page)\n",
    "            logger.info(\n",
    "                f\"Página {self.current_page} marcada como primera página\")\n",
    "\n",
    "        self.next_page()\n",
    "\n",
    "    def next_page(self):\n",
    "        \"\"\"Pasa a la siguiente página\"\"\"\n",
    "        self.current_page += 1\n",
    "        pages = os.listdir(os.path.join(self.processed_dir, self.current_pdf))\n",
    "\n",
    "        if self.current_page >= len(pages):\n",
    "            self.labels[self.current_pdf][\"total_pages\"] = len(pages)\n",
    "            self.save_labels()\n",
    "            self.load_next_pdf()\n",
    "        else:\n",
    "            self.show_current_page()\n",
    "\n",
    "    def load_next_pdf(self):\n",
    "        \"\"\"Carga el siguiente PDF no etiquetado\"\"\"\n",
    "        self.current_pdf_index += 1\n",
    "\n",
    "        if self.current_pdf_index < len(self.pdfs):\n",
    "            self.current_pdf = self.pdfs[self.current_pdf_index]\n",
    "            self.current_page = 0\n",
    "            self.show_current_page()\n",
    "        else:\n",
    "            logger.info(\"Todos los PDFs han sido etiquetados\")\n",
    "            self.save_and_exit()\n",
    "\n",
    "    def save_labels(self):\n",
    "        \"\"\"Guarda las etiquetas en el archivo\"\"\"\n",
    "        try:\n",
    "            with open(self.labels_file, 'w') as f:\n",
    "                json.dump(self.labels, f, indent=4)\n",
    "            logger.info(\"Etiquetas guardadas correctamente\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando etiquetas: {e}\")\n",
    "\n",
    "    def save_and_exit(self):\n",
    "        \"\"\"Guarda las etiquetas y cierra la aplicación\"\"\"\n",
    "        self.save_labels()\n",
    "        self.window.after(1000, self.window.quit)  # Esperar un segundo antes de cerrar\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    labeler = PDFLabeler()\n",
    "    labeler.start_labeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando Spark...\n",
      "Spark configurado correctamente.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Leer etiquetas desde el archivo JSON\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(LABELS_JSON, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m---> 36\u001b[0m     labels \u001b[38;5;241m=\u001b[39m \u001b[43mjson\u001b[49m\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Crear DataFrame de imágenes y etiquetas\u001b[39;00m\n\u001b[0;32m     39\u001b[0m image_data \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# Paso 4: Crear un DataFrame de Spark con las Imágenes y Etiquetas\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Configurar Spark\n",
    "print(\"Configurando Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ImageProcessing\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark configurado correctamente.\")\n",
    "\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_JSON = 'data/labels.json'\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocesa una imagen para el modelo\"\"\"\n",
    "    try:\n",
    "        print(f\"Preprocesando imagen {image_path}...\")\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized.flatten().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocesando imagen {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Leer etiquetas desde el archivo JSON\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Crear DataFrame de imágenes y etiquetas\n",
    "image_data = []\n",
    "for pdf_name, pdf_info in labels.items():\n",
    "    for page_num in range(pdf_info[\"total_pages\"]):\n",
    "        image_path = os.path.join(PROCESSED_DIR, pdf_name, f'page_{page_num}.png')\n",
    "        label = 1 if page_num in pdf_info[\"target_pages\"] else 0\n",
    "        features = preprocess_image(image_path)\n",
    "        if features is not None:\n",
    "            image_data.append((image_path, label, features))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"features\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(image_data, schema)\n",
    "print(\"DataFrame creado correctamente.\")\n",
    "\n",
    "# Guardar DataFrame procesado\n",
    "output_path = os.path.join(PROCESSED_DIR, \"processed_images_with_labels.parquet\")\n",
    "print(f\"Guardando DataFrame en {output_path}...\")\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(\"DataFrame guardado correctamente.\")\n",
    "\n",
    "# Mostrar estadísticas del procesamiento\n",
    "print(f\"Total de imágenes procesadas: {df.count()}\")\n",
    "print(\"\\nDistribución de etiquetas:\")\n",
    "df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Verificar valores nulos o problemas\n",
    "print(\"\\nVerificación de calidad:\")\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros en el conjunto de entrenamiento: 304\n",
      "Total de registros en el conjunto de prueba: 66\n",
      "\n",
      "Métricas del modelo:\n",
      "Accuracy: 0.8939\n",
      "\n",
      "Estadísticas del procesamiento:\n",
      "Total imágenes: 370\n",
      "\n",
      "Distribución de clases:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  325|\n",
      "|    1|   45|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configuración de rutas\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_JSON = 'data/labels.json'\n",
    "MODEL_DIR = 'data/models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DocumentClassifier\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"No se pudo cargar la imagen: {image_path}\")\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized.flatten().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cargar y procesar datos\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "image_data = []\n",
    "for pdf_name, pdf_info in labels.items():\n",
    "    for page_num in range(pdf_info[\"total_pages\"]):\n",
    "        image_path = os.path.join(PROCESSED_DIR, pdf_name, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            label = 1 if page_num in pdf_info[\"target_pages\"] else 0\n",
    "            features = preprocess_image(image_path)\n",
    "            if features is not None:\n",
    "                image_data.append((image_path, label, features))\n",
    "\n",
    "# Crear DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"features\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "# Convertir features a vector\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler(inputCol=\"features_vector\", \n",
    "                       outputCol=\"scaled_features\",\n",
    "                       withStd=True,\n",
    "                       withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "df_scaled = scaler_model.transform(df)\n",
    "\n",
    "# Preparar datos para entrenamiento\n",
    "final_df = df_scaled.select(\"label\", \"scaled_features\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Verificar datos antes de entrenar\n",
    "print(f\"Total de registros en el conjunto de entrenamiento: {train_df.count()}\")\n",
    "print(f\"Total de registros en el conjunto de prueba: {test_df.count()}\")\n",
    "\n",
    "# Entrenar modelo\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "try:\n",
    "    lr_model = lr.fit(train_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento del modelo: {str(e)}\")\n",
    "    spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "# Evaluar modelo\n",
    "predictions = lr_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Métricas\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"\\nMétricas del modelo:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Guardar modelo y datos procesados\n",
    "lr_model.save(os.path.join(MODEL_DIR, \"logistic_regression_model\"))\n",
    "df_scaled.write.mode(\"overwrite\").parquet(os.path.join(PROCESSED_DIR, \"processed_scaled_data.parquet\"))\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(\"\\nEstadísticas del procesamiento:\")\n",
    "print(f\"Total imágenes: {df.count()}\")\n",
    "print(\"\\nDistribución de clases:\")\n",
    "df.groupBy(\"label\").count().show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  325|\n",
      "|    1|  325|\n",
      "+-----+-----+\n",
      "\n",
      "Total de registros en el conjunto de entrenamiento: 521\n",
      "Total de registros en el conjunto de prueba: 129\n",
      "\n",
      "Métricas del modelo:\n",
      "Accuracy: 0.9612\n",
      "\n",
      "Estadísticas del procesamiento:\n",
      "Total imágenes: 370\n",
      "\n",
      "Distribución de clases:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  325|\n",
      "|    1|   45|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configuración de rutas\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_JSON = 'data/labels.json'\n",
    "MODEL_DIR = 'data/models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DocumentClassifier\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"No se pudo cargar la imagen: {image_path}\")\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized.flatten().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cargar y procesar datos\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "image_data = []\n",
    "for pdf_name, pdf_info in labels.items():\n",
    "    for page_num in range(pdf_info[\"total_pages\"]):\n",
    "        image_path = os.path.join(PROCESSED_DIR, pdf_name, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            label = 1 if page_num in pdf_info[\"target_pages\"] else 0\n",
    "            features = preprocess_image(image_path)\n",
    "            if features is not None:\n",
    "                image_data.append((image_path, label, features))\n",
    "\n",
    "# Crear DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"features\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "# Convertir features a vector\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler(inputCol=\"features_vector\", \n",
    "                       outputCol=\"scaled_features\",\n",
    "                       withStd=True,\n",
    "                       withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "df_scaled = scaler_model.transform(df)\n",
    "\n",
    "# Sobremuestreo de la clase minoritaria\n",
    "minority_class_df = df_scaled.filter(col(\"label\") == 1)\n",
    "majority_class_df = df_scaled.filter(col(\"label\") == 0)\n",
    "\n",
    "# Número de ejemplos en la clase mayoritaria\n",
    "majority_count = majority_class_df.count()\n",
    "\n",
    "# Sobremuestrear la clase minoritaria\n",
    "oversampled_minority_class_df = minority_class_df.sample(withReplacement=True, fraction=majority_count / minority_class_df.count())\n",
    "\n",
    "# Combinar los DataFrames\n",
    "balanced_df = majority_class_df.union(oversampled_minority_class_df)\n",
    "\n",
    "# Verificar la nueva distribución de clases\n",
    "balanced_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Preparar datos para entrenamiento\n",
    "final_df = balanced_df.select(\"label\", \"scaled_features\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Verificar datos antes de entrenar\n",
    "print(f\"Total de registros en el conjunto de entrenamiento: {train_df.count()}\")\n",
    "print(f\"Total de registros en el conjunto de prueba: {test_df.count()}\")\n",
    "\n",
    "# Entrenar modelo\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "try:\n",
    "    lr_model = lr.fit(train_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento del modelo: {str(e)}\")\n",
    "    spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "# Evaluar modelo\n",
    "predictions = lr_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Métricas\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"\\nMétricas del modelo:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Guardar modelo y datos procesados\n",
    "try:\n",
    "    lr_model.write().overwrite().save(os.path.join(MODEL_DIR, \"logistic_regression_model\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar el modelo: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    df_scaled.write.mode(\"overwrite\").parquet(os.path.join(PROCESSED_DIR, \"processed_scaled_data.parquet\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar los datos procesados: {str(e)}\")\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(\"\\nEstadísticas del procesamiento:\")\n",
    "print(f\"Total imágenes: {df.count()}\")\n",
    "print(\"\\nDistribución de clases:\")\n",
    "df.groupBy(\"label\").count().show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  325|\n",
      "|    1|  346|\n",
      "+-----+-----+\n",
      "\n",
      "Total de registros en el conjunto de entrenamiento: 540\n",
      "Total de registros en el conjunto de prueba: 131\n",
      "\n",
      "Métricas del modelo:\n",
      "Accuracy: 0.9466\n",
      "Precision: 0.9496\n",
      "Recall: 0.9466\n",
      "F1-score: 0.9467\n",
      "\n",
      "Estadísticas del procesamiento:\n",
      "Total imágenes: 370\n",
      "\n",
      "Distribución de clases:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|  325|\n",
      "|    1|   45|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configuración de rutas\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_JSON = 'data/labels.json'\n",
    "MODEL_DIR = 'data/models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DocumentClassifier\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"No se pudo cargar la imagen: {image_path}\")\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized.flatten().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cargar y procesar datos\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "image_data = []\n",
    "for pdf_name, pdf_info in labels.items():\n",
    "    for page_num in range(pdf_info[\"total_pages\"]):\n",
    "        image_path = os.path.join(PROCESSED_DIR, pdf_name, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            label = 1 if page_num in pdf_info[\"target_pages\"] else 0\n",
    "            features = preprocess_image(image_path)\n",
    "            if features is not None:\n",
    "                image_data.append((image_path, label, features))\n",
    "\n",
    "# Crear DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"features\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "# Convertir features a vector\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler(inputCol=\"features_vector\", \n",
    "                       outputCol=\"scaled_features\",\n",
    "                       withStd=True,\n",
    "                       withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "df_scaled = scaler_model.transform(df)\n",
    "\n",
    "# Guardar el escalador\n",
    "try:\n",
    "    scaler_model.write().overwrite().save(os.path.join(PROCESSED_DIR, \"scaler_model\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar el escalador: {str(e)}\")\n",
    "\n",
    "# Sobremuestreo de la clase minoritaria\n",
    "minority_class_df = df_scaled.filter(col(\"label\") == 1)\n",
    "majority_class_df = df_scaled.filter(col(\"label\") == 0)\n",
    "\n",
    "# Número de ejemplos en la clase mayoritaria\n",
    "majority_count = majority_class_df.count()\n",
    "\n",
    "# Sobremuestrear la clase minoritaria\n",
    "oversampled_minority_class_df = minority_class_df.sample(withReplacement=True, fraction=majority_count / minority_class_df.count())\n",
    "\n",
    "# Combinar los DataFrames\n",
    "balanced_df = majority_class_df.union(oversampled_minority_class_df)\n",
    "\n",
    "# Verificar la nueva distribución de clases\n",
    "balanced_df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Preparar datos para entrenamiento\n",
    "final_df = balanced_df.select(\"label\", \"scaled_features\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Verificar datos antes de entrenar\n",
    "print(f\"Total de registros en el conjunto de entrenamiento: {train_df.count()}\")\n",
    "print(f\"Total de registros en el conjunto de prueba: {test_df.count()}\")\n",
    "\n",
    "# Entrenar modelo\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "try:\n",
    "    lr_model = lr.fit(train_df)\n",
    "except Exception as e:\n",
    "    print(f\"Error durante el entrenamiento del modelo: {str(e)}\")\n",
    "    spark.stop()\n",
    "    sys.exit(1)\n",
    "\n",
    "# Evaluar modelo\n",
    "predictions = lr_model.transform(test_df)\n",
    "\n",
    "# Evaluar con métricas adicionales\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "accuracy = evaluator_accuracy.evaluate(predictions)\n",
    "precision = evaluator_precision.evaluate(predictions)\n",
    "recall = evaluator_recall.evaluate(predictions)\n",
    "f1 = evaluator_f1.evaluate(predictions)\n",
    "\n",
    "print(f\"\\nMétricas del modelo:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-score: {f1:.4f}\")\n",
    "\n",
    "# Guardar modelo y datos procesados\n",
    "try:\n",
    "    lr_model.write().overwrite().save(os.path.join(MODEL_DIR, \"logistic_regression_model\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar el modelo: {str(e)}\")\n",
    "\n",
    "try:\n",
    "    df_scaled.write.mode(\"overwrite\").parquet(os.path.join(PROCESSED_DIR, \"processed_scaled_data.parquet\"))\n",
    "except Exception as e:\n",
    "    print(f\"Error al guardar los datos procesados: {str(e)}\")\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(\"\\nEstadísticas del procesamiento:\")\n",
    "print(f\"Total imágenes: {df.count()}\")\n",
    "print(\"\\nDistribución de clases:\")\n",
    "df.groupBy(\"label\").count().show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n",
      "CORRECTO: el proceso con PID 23144 (proceso secundario de PID 16040)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 16040 (proceso secundario de PID 3188)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 3188 (proceso secundario de PID 23072)\n",
      "ha sido terminado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/04 23:57:47 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 122, in <module>\n",
      "    predict_first_pages(pdf_dir, model_path, scaler_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 77, in predict_first_pages\n",
      "    lr_model = LogisticRegressionModel.load(model_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 369, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 318, in load\n",
      "    java_obj = self._jread.load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o35.load.\n",
      ": java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\n",
      "\n",
      "\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
      "\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run.py predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o549.load.\n: java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 122\u001b[0m\n\u001b[0;32m    120\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/models/logistic_regression_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    121\u001b[0m scaler_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/scaler_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 122\u001b[0m \u001b[43mpredict_first_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[1;32mIn[7], line 77\u001b[0m, in \u001b[0;36mpredict_first_pages\u001b[1;34m(pdf_dir, model_path, scaler_path)\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;66;03m# Cargar el modelo y el escalador\u001b[39;00m\n\u001b[1;32m---> 77\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegressionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m scaler_model \u001b[38;5;241m=\u001b[39m StandardScalerModel\u001b[38;5;241m.\u001b[39mload(scaler_path)\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Procesar las páginas del PDF\u001b[39;00m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py:318\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[1;32m--> 318\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o549.load.\n: java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    # Verificar las variables de entorno\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    java_home = os.environ.get('JAVA_HOME')\n",
    "    \n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "    \n",
    "    if not java_home:\n",
    "        print(\"Error: JAVA_HOME no está configurado.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar la existencia de las bibliotecas nativas de Hadoop\n",
    "    native_lib_path = os.path.join(hadoop_home, 'bin')\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(os.path.join(native_lib_path, 'hadoop.dll'))\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Todas las bibliotecas necesarias de Hadoop están presentes.\")\n",
    "    return True\n",
    "\n",
    "# Verificar las bibliotecas necesarias de Hadoop antes de ejecutar el script\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configurar las bibliotecas nativas de Hadoop\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path):\n",
    "    # Verificar si el modelo y el escalador existen\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    # Cargar el modelo y el escalador\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    # Procesar las páginas del PDF\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            # Leer la imagen y convertirla a un vector\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                image_data.append((image_path, features))\n",
    "\n",
    "    # Crear DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    # Convertir features a vector\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    # Escalar características\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    # Filtrar las primeras páginas predichas\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_dir = 'data/pdf_pages'\n",
    "model_path = 'data/models/logistic_regression_model'\n",
    "scaler_path = 'data/processed/scaler_model'\n",
    "predict_first_pages(pdf_dir, model_path, scaler_path)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o553.load.\n: java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 121\u001b[0m\n\u001b[0;32m    119\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/models/logistic_regression_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    120\u001b[0m scaler_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/processed/scaler_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m--> 121\u001b[0m \u001b[43mpredict_first_pages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m spark\u001b[38;5;241m.\u001b[39mstop()\n",
      "Cell \u001b[1;32mIn[8], line 76\u001b[0m, in \u001b[0;36mpredict_first_pages\u001b[1;34m(pdf_dir, model_path, scaler_path)\u001b[0m\n\u001b[0;32m     73\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Cargar el modelo y el escalador\u001b[39;00m\n\u001b[1;32m---> 76\u001b[0m lr_model \u001b[38;5;241m=\u001b[39m \u001b[43mLogisticRegressionModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m scaler_model \u001b[38;5;241m=\u001b[39m StandardScalerModel\u001b[38;5;241m.\u001b[39mload(scaler_path)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# Procesar las páginas del PDF\u001b[39;00m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py:369\u001b[0m, in \u001b[0;36mMLReadable.load\u001b[1;34m(cls, path)\u001b[0m\n\u001b[0;32m    366\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    367\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, path: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m RL:\n\u001b[0;32m    368\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Reads an ML instance from the input path, a shortcut of `read().load(path)`.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 369\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py:318\u001b[0m, in \u001b[0;36mJavaMLReader.load\u001b[1;34m(self, path)\u001b[0m\n\u001b[0;32m    316\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    317\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath should be a string, got type \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(path))\n\u001b[1;32m--> 318\u001b[0m java_obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_from_java\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    320\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\n\u001b[0;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis Java ML type cannot be loaded into Python currently: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clazz\n\u001b[0;32m    322\u001b[0m     )\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o553.load.\n: java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\r\n\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\r\n\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\r\n\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\r\n\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\r\n\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\r\n\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    # Verificar las variables de entorno\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    java_home = os.environ.get('JAVA_HOME')\n",
    "    \n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "    \n",
    "    if not java_home:\n",
    "        print(\"Error: JAVA_HOME no está configurado.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "    \n",
    "    # Verificar la existencia de las bibliotecas nativas de Hadoop\n",
    "    native_lib_path = os.path.join(hadoop_home, 'bin')\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(os.path.join(native_lib_path, 'hadoop.dll'))\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "    \n",
    "    print(\"Todas las bibliotecas necesarias de Hadoop están presentes.\")\n",
    "    return True\n",
    "\n",
    "# Verificar las bibliotecas necesarias de Hadoop antes de ejecutar el script\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configurar las bibliotecas nativas de Hadoop\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "# Inicializar Spark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path):\n",
    "    # Verificar si el modelo y el escalador existen\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    # Cargar el modelo y el escalador\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    # Procesar las páginas del PDF\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            # Leer la imagen y convertirla a un vector\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                image_data.append((image_path, features))\n",
    "\n",
    "    # Crear DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    # Convertir features a vector\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    # Escalar características\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    # Filtrar las primeras páginas predichas\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_dir = 'data/pdf_pages'\n",
    "model_path = 'data/models/logistic_regression_model'\n",
    "scaler_path = 'data/processed/scaler_model'\n",
    "predict_first_pages(pdf_dir, model_path, scaler_path)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop version: 3.3.4\n",
      "Python version: 3.10.11 (tags/v3.10.11:7d4cc5a, Apr  5 2023, 00:38:17) [MSC v.1929 64 bit (AMD64)]\n",
      "Spark version: 3.5.4\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"CheckHadoopVersion\").getOrCreate()\n",
    "\n",
    "# Obtener la versión de Hadoop\n",
    "hadoop_version = spark.sparkContext._jvm.org.apache.hadoop.util.VersionInfo.getVersion()\n",
    "print(\"Hadoop version:\", hadoop_version)\n",
    "\n",
    "# Detener la sesión de Spark\n",
    "spark.stop()\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "print(\"Python version:\", sys.version)\n",
    "\n",
    "# %%\n",
    "import pyspark\n",
    "print(\"Spark version:\", pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME: C:\\Program Files\\winutils\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n",
      "Rutas en PATH:\n",
      "E:\\proyectos\\validarpdf\\document_classifier\\.venv\\Lib\\site-packages\\cv2\\../../x64/vc14/bin\n",
      "e:\\proyectos\\validarpdf\\document_classifier\\.venv\\Scripts\n",
      "C:\\Program Files\\Common Files\\Oracle\\Java\\javapath\n",
      "C:\\Program Files\\NVIDIA\\CUDNN\\v9.5\\bin\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\bin\n",
      "C:\\Program Files\\NVIDIA GPU Computing Toolkit\\CUDA\\v12.6\\libnvvp\n",
      "\n",
      "\n",
      "\n",
      "C:\\Program Files\\Python312\\Scripts\\\n",
      "C:\\Program Files\\Python312\\\n",
      "C:\\WINDOWS\\system32\n",
      "C:\\WINDOWS\n",
      "C:\\WINDOWS\\System32\\Wbem\n",
      "C:\\WINDOWS\\System32\\WindowsPowerShell\\v1.0\\\n",
      "C:\\WINDOWS\\System32\\OpenSSH\\\n",
      "C:\\Program Files\\Git\\cmd\n",
      "C:\\Program Files\\nodejs\\\n",
      "C:\\ProgramData\\chocolatey\\bin\n",
      "C:\\Program Files\\Microsoft SQL Server\\Client SDK\\ODBC\\170\\Tools\\Binn\\\n",
      "C:\\Program Files (x86)\\Microsoft SQL Server\\150\\Tools\\Binn\\\n",
      "C:\\Program Files\\Microsoft SQL Server\\150\\Tools\\Binn\\\n",
      "C:\\Program Files\\Microsoft SQL Server\\150\\DTS\\Binn\\\n",
      "C:\\Program Files (x86)\\Microsoft SQL Server\\160\\DTS\\Binn\\\n",
      "C:\\Program Files\\Azure Data Studio\\bin\n",
      "C:\\Program Files\\Amazon\\AWSCLIV2\\\n",
      "C:\\Program Files\\QGIS 3.34.12\\apps\\otb\\bin\n",
      "C:\\Program Files\\dotnet\\\n",
      "C:\\Program Files\\NVIDIA Corporation\\Nsight Compute 2024.3.2\\\n",
      "C:\\Program Files (x86)\\NVIDIA Corporation\\PhysX\\Common\n",
      "\n",
      "C:\\Program Files\\NVIDIA Corporation\\NVIDIA app\\NvDLISR\n",
      "C:\\Program Files (x86)\\Windows Kits\\10\\Windows Performance Toolkit\\\n",
      "C:\\Users\\afran\\AppData\\Local\\Programs\\Python\\Python313\\Scripts\\\n",
      "C:\\Users\\afran\\AppData\\Local\\Programs\\Python\\Python313\\\n",
      "C:\\Users\\afran\\.console-ninja\\.bin\n",
      "C:\\Users\\afran\\AppData\\Local\\Microsoft\\WindowsApps\n",
      "C:\\Users\\afran\\AppData\\Local\\Programs\\Microsoft VS Code\\bin\n",
      "C:\\Users\\afran\\AppData\\Roaming\\npm\n",
      "C:\\Program Files\\Azure Data Studio\\bin\n",
      "C:\\Program Files\\Java\\jdk-21\\bin\n",
      "C:\\Program Files\\spark\\bin\n",
      "C:\\Program Files\\winutils\\bin\n",
      "C:\\ProgramData\\anaconda3\n",
      "C:\\ProgramData\\anaconda3\\Scripts\n",
      "C:\\ProgramData\\anaconda3\\Library\\bin\n",
      "C:\\Users\\afran\\.dotnet\\tools\n",
      "C:\\Users\\afran\\AppData\\Local\\Microsoft\\WindowsApps\n",
      "C:\\Program Files\\QGIS 3.34.12\\bin\n",
      "C:\\Program Files\\QGIS 3.34.12\\apps\\qgis-ltr\\bin\n",
      "C:\\Program Files\\QGIS 3.34.12\\apps\\Python312\n",
      "C:\\Program Files\\QGIS 3.34.12\\apps\\Python312\\Scripts\n",
      "\n",
      "\n",
      "Configuración de Spark:\n",
      "('spark.driver.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.driver.memory', '4g')\n",
      "('spark.executor.memory', '4g')\n",
      "('spark.executor.id', 'driver')\n",
      "('spark.driver.port', '58971')\n",
      "('spark.app.submitTime', '1738722549498')\n",
      "('spark.app.name', 'CheckHadoopConfig')\n",
      "('spark.app.id', 'local-1738727785698')\n",
      "('spark.rdd.compress', 'True')\n",
      "('spark.executor.extraJavaOptions', '-Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false')\n",
      "('spark.app.startTime', '1738727785648')\n",
      "('spark.serializer.objectStreamReset', '100')\n",
      "('spark.master', 'local[*]')\n",
      "('spark.submit.pyFiles', '')\n",
      "('spark.submit.deployMode', 'client')\n",
      "('spark.driver.host', 'Loboreapper')\n",
      "('spark.ui.showConsoleProgress', 'true')\n"
     ]
    }
   ],
   "source": [
    "# Verificar la Ruta de Hadoop y Java\n",
    "import os\n",
    "\n",
    "# Verificar la variable de entorno HADOOP_HOME\n",
    "hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "if hadoop_home:\n",
    "    print(f\"HADOOP_HOME: {hadoop_home}\")\n",
    "else:\n",
    "    print(\"HADOOP_HOME no está configurado.\")\n",
    "\n",
    "# Verificar la variable de entorno JAVA_HOME\n",
    "java_home = os.environ.get('JAVA_HOME')\n",
    "if java_home:\n",
    "    print(f\"JAVA_HOME: {java_home}\")\n",
    "else:\n",
    "    print(\"JAVA_HOME no está configurado.\")\n",
    "\n",
    "# Verificar la ruta de Hadoop en el PATH\n",
    "path = os.environ.get('PATH')\n",
    "if path:\n",
    "    print(\"Rutas en PATH:\")\n",
    "    for p in path.split(os.pathsep):\n",
    "        print(p)\n",
    "else:\n",
    "    print(\"PATH no está configurado.\")\n",
    "\n",
    "# Verificar la Configuración de Spark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear una sesión de Spark\n",
    "spark = SparkSession.builder.appName(\"CheckHadoopConfig\").getOrCreate()\n",
    "\n",
    "# Verificar la configuración de Spark\n",
    "spark_conf = spark.sparkContext.getConf().getAll()\n",
    "print(\"\\nConfiguración de Spark:\")\n",
    "for conf in spark_conf:\n",
    "    print(conf)\n",
    "\n",
    "# Detener la sesión de Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CORRECTO: el proceso con PID 6960 (proceso secundario de PID 17960)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 17960 (proceso secundario de PID 21840)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 21840 (proceso secundario de PID 4560)\n",
      "ha sido terminado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 80, in <module>\n",
      "    predict_first_pages(pdf_dir, model_path, scaler_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 39, in predict_first_pages\n",
      "    lr_model = LogisticRegressionModel.load(model_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 369, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 318, in load\n",
      "    java_obj = self._jread.load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o33.load.\n",
      ": java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\n",
      "\n",
      "\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
      "\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run.py predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "# echo $SPARK_HOME  # En Linux/Mac\n",
    "!echo %SPARK_HOME% \n",
    "# En Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import numpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\steev\\documents\\airanfranco\\tributai  ia\\document_classifier\\.venv\\lib\\site-packages (2.0.2)\n",
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
