{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparación de Datos con PySpark\n",
    "\n",
    "Este notebook realiza la preparación de datos usando PySpark para procesar documentos PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando Spark...\n",
      "Spark configurado correctamente.\n",
      "Directorio data/raw ya existe.\n",
      "Directorio data/processed ya existe.\n",
      "Directorios configurados: RAW_DIR=data/raw, PROCESSED_DIR=data/processed\n"
     ]
    }
   ],
   "source": [
    "# Configuración inicial\n",
    "import os\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import udf, col\n",
    "import numpy as np\n",
    "import subprocess\n",
    "\n",
    "# Instalar pdf2image si no está disponible\n",
    "try:\n",
    "    import pdf2image\n",
    "except ImportError:\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"pdf2image\"])\n",
    "finally:\n",
    "    from pdf2image import convert_from_path\n",
    "\n",
    "import cv2\n",
    "\n",
    "# Configurar Spark\n",
    "print(\"Configurando Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFPreprocessing\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark configurado correctamente.\")\n",
    "\n",
    "# Configurar directorios\n",
    "RAW_DIR = 'data/raw'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "\n",
    "# Validar y crear directorios si no existen\n",
    "if not os.path.exists(RAW_DIR):\n",
    "    print(f\"Directorio {RAW_DIR} no existe. Creándolo...\")\n",
    "    os.makedirs(RAW_DIR, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directorio {RAW_DIR} ya existe.\")\n",
    "\n",
    "if not os.path.exists(PROCESSED_DIR):\n",
    "    print(f\"Directorio {PROCESSED_DIR} no existe. Creándolo...\")\n",
    "    os.makedirs(PROCESSED_DIR, exist_ok=True)\n",
    "else:\n",
    "    print(f\"Directorio {PROCESSED_DIR} ya existe.\")\n",
    "\n",
    "print(f\"Directorios configurados: RAW_DIR={RAW_DIR}, PROCESSED_DIR={PROCESSED_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando data/raw\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30.pdf...\n",
      "Guardada página 0 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_0.png\n",
      "Guardada página 1 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_1.png\n",
      "Guardada página 2 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_2.png\n",
      "Guardada página 3 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_3.png\n",
      "Guardada página 4 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_4.png\n",
      "Guardada página 5 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_5.png\n",
      "Guardada página 6 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_6.png\n",
      "Guardada página 7 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_7.png\n",
      "Guardada página 8 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_8.png\n",
      "Guardada página 9 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_9.png\n",
      "Guardada página 10 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_10.png\n",
      "Guardada página 11 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_11.png\n",
      "Guardada página 12 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_12.png\n",
      "Guardada página 13 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_13.png\n",
      "Guardada página 14 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_14.png\n",
      "Guardada página 15 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_15.png\n",
      "Guardada página 16 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_16.png\n",
      "Guardada página 17 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_17.png\n",
      "Guardada página 18 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_18.png\n",
      "Guardada página 19 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_19.png\n",
      "Guardada página 20 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_20.png\n",
      "Guardada página 21 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_21.png\n",
      "Guardada página 22 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_22.png\n",
      "Guardada página 23 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_23.png\n",
      "Guardada página 24 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_24.png\n",
      "Guardada página 25 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_25.png\n",
      "Guardada página 26 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_26.png\n",
      "Guardada página 27 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_27.png\n",
      "Guardada página 28 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_28.png\n",
      "Guardada página 29 de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 en data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_29.png\n",
      "Procesamiento de 2-TITULOS-15-DE-NOVIEMBRE-2024-1-30 completado.\n"
     ]
    }
   ],
   "source": [
    "# Procesar PDFs a imágenes\n",
    "def process_pdf(pdf_path):\n",
    "    \"\"\"Procesa un PDF y guarda sus páginas como imágenes\"\"\"\n",
    "    try:\n",
    "        print(f\"Procesando {pdf_path}...\")\n",
    "        # Convertir PDF a imágenes\n",
    "        pages = convert_from_path(pdf_path)\n",
    "        pdf_name = os.path.basename(pdf_path).replace('.pdf', '')\n",
    "        \n",
    "        # Crear directorio para este PDF\n",
    "        pdf_dir = os.path.join(PROCESSED_DIR, pdf_name)\n",
    "        os.makedirs(pdf_dir, exist_ok=True)\n",
    "        \n",
    "        # Guardar cada página como imagen\n",
    "        for i, page in enumerate(pages):\n",
    "            page_path = os.path.join(pdf_dir, f'page_{i}.png')\n",
    "            page.save(page_path)\n",
    "            print(f\"Guardada página {i} de {pdf_name} en {page_path}\")\n",
    "            \n",
    "        print(f\"Procesamiento de {pdf_name} completado.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {pdf_path}: {str(e)}\")\n",
    "\n",
    "# Procesar todos los PDFs en el directorio RAW_DIR\n",
    "pdf_files = [os.path.join(RAW_DIR, f) for f in os.listdir(RAW_DIR) if f.endswith('.pdf')]\n",
    "for pdf_file in pdf_files:\n",
    "    process_pdf(pdf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Etiquetas guardadas correctamente\n",
      "ERROR:__main__:Error mostrando página: No se encuentra la página: data/processed\\processed_images.parquet\\page_0.png\n"
     ]
    }
   ],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk\n",
    "from PIL import Image, ImageTk\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class PDFLabeler:\n",
    "    def __init__(self, processed_dir='data/processed', labels_file='data/labels.json'):\n",
    "        self.processed_dir = processed_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.window = tk.Tk()\n",
    "        self.window.title(\"PDF Page Labeler\")\n",
    "        self.setup_gui()\n",
    "        self.load_existing_labels()\n",
    "\n",
    "    def setup_gui(self):\n",
    "        \"\"\"Configura la interfaz gráfica\"\"\"\n",
    "        # Frame principal\n",
    "        self.main_frame = ttk.Frame(self.window, padding=\"10\")\n",
    "        self.main_frame.grid(row=0, column=0, sticky=(tk.W, tk.E, tk.N, tk.S))\n",
    "\n",
    "        # Área de visualización\n",
    "        self.image_label = ttk.Label(self.main_frame)\n",
    "        self.image_label.grid(row=0, column=0, columnspan=3, pady=5)\n",
    "\n",
    "        # Controles\n",
    "        ttk.Label(self.main_frame, text=\"PDF actual:\").grid(\n",
    "            row=1, column=0, sticky=tk.W)\n",
    "        self.pdf_label = ttk.Label(self.main_frame, text=\"\")\n",
    "        self.pdf_label.grid(row=1, column=1, columnspan=2, sticky=tk.W)\n",
    "\n",
    "        ttk.Label(self.main_frame, text=\"Página:\").grid(\n",
    "            row=2, column=0, sticky=tk.W)\n",
    "        self.page_label = ttk.Label(self.main_frame, text=\"\")\n",
    "        self.page_label.grid(row=2, column=1, columnspan=2, sticky=tk.W)\n",
    "\n",
    "        # Botones\n",
    "        ttk.Button(self.main_frame, text=\"Es Primera Página\",\n",
    "                   command=self.mark_first_page).grid(row=3, column=0, pady=5)\n",
    "        ttk.Button(self.main_frame, text=\"No es Primera Página\",\n",
    "                   command=self.next_page).grid(row=3, column=1, pady=5)\n",
    "        ttk.Button(self.main_frame, text=\"Guardar y Salir\",\n",
    "                   command=self.save_and_exit).grid(row=3, column=2, pady=5)\n",
    "        # Agregar botones de navegación\n",
    "        nav_frame = ttk.Frame(self.main_frame)\n",
    "        nav_frame.grid(row=4, column=0, columnspan=3, pady=5)\n",
    "\n",
    "        ttk.Button(nav_frame, text=\"← Anterior\",\n",
    "                   command=self.previous_page).grid(row=0, column=0, padx=5)\n",
    "        ttk.Button(nav_frame, text=\"Siguiente →\",\n",
    "                   command=self.next_page).grid(row=0, column=1, padx=5)\n",
    "\n",
    "        # Mostrar estado de etiquetado\n",
    "        self.status_label = ttk.Label(self.main_frame, text=\"\")\n",
    "        self.status_label.grid(row=5, column=0, columnspan=3, pady=5)\n",
    "\n",
    "        # Agregar atajos de teclado\n",
    "        self.window.bind('<Left>', lambda e: self.previous_page())\n",
    "        self.window.bind('<Right>', lambda e: self.next_page())\n",
    "        self.window.bind('<space>', lambda e: self.mark_first_page())\n",
    "        self.window.bind('q', lambda e: self.save_and_exit())\n",
    "\n",
    "    def previous_page(self):\n",
    "        \"\"\"Retrocede a la página anterior\"\"\"\n",
    "        if self.current_page > 0:\n",
    "            self.current_page -= 1\n",
    "            self.show_current_page()\n",
    "\n",
    "    def show_current_page(self):\n",
    "        \"\"\"Muestra la página actual\"\"\"\n",
    "        try:\n",
    "            if self.current_pdf_index >= len(self.pdfs):\n",
    "                return  # No intentar mostrar una página si ya hemos terminado\n",
    "\n",
    "            page_path = os.path.join(\n",
    "                self.processed_dir,\n",
    "                self.current_pdf,\n",
    "                f'page_{self.current_page}.png'\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(page_path):\n",
    "                raise FileNotFoundError(\n",
    "                    f\"No se encuentra la página: {page_path}\")\n",
    "\n",
    "            # Cargar y mostrar imagen\n",
    "            image = Image.open(page_path)\n",
    "\n",
    "            # Mantener proporción de aspecto\n",
    "            width, height = image.size\n",
    "            ratio = min(800/width, 600/height)\n",
    "            new_size = (int(width * ratio), int(height * ratio))\n",
    "            image = image.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "            photo = ImageTk.PhotoImage(image)\n",
    "            self.image_label.configure(image=photo)\n",
    "            self.image_label.image = photo\n",
    "\n",
    "            # Actualizar etiquetas\n",
    "            self.pdf_label.configure(text=self.current_pdf)\n",
    "            self.page_label.configure(\n",
    "                text=f\"Página {self.current_page + 1} de {self.get_total_pages()}\")\n",
    "\n",
    "            # Actualizar estado\n",
    "            is_marked = self.is_page_marked(self.current_page)\n",
    "            status = \"MARCADA COMO PRIMERA PÁGINA\" if is_marked else \"NO MARCADA\"\n",
    "            self.status_label.configure(text=f\"Estado: {status}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error mostrando página: {e}\")\n",
    "            self.status_label.configure(text=\"Error mostrando la página\")\n",
    "\n",
    "    def get_total_pages(self):\n",
    "        \"\"\"Obtiene el total de páginas del PDF actual\"\"\"\n",
    "        pdf_dir = os.path.join(self.processed_dir, self.current_pdf)\n",
    "        return len([f for f in os.listdir(pdf_dir) if f.endswith('.png')])\n",
    "\n",
    "    def is_page_marked(self, page_num):\n",
    "        \"\"\"Verifica si la página está marcada como primera página\"\"\"\n",
    "        return (self.current_pdf in self.labels and\n",
    "                page_num in self.labels[self.current_pdf][\"target_pages\"])\n",
    "\n",
    "    def load_existing_labels(self):\n",
    "        \"\"\"Carga etiquetas existentes si las hay\"\"\"\n",
    "        try:\n",
    "            if os.path.exists(self.labels_file):\n",
    "                with open(self.labels_file, 'r') as f:\n",
    "                    self.labels = json.load(f)\n",
    "            else:\n",
    "                self.labels = {}\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error cargando etiquetas: {e}\")\n",
    "            self.labels = {}\n",
    "\n",
    "    def start_labeling(self):\n",
    "        \"\"\"Inicia el proceso de etiquetado\"\"\"\n",
    "        pdfs = [d for d in os.listdir(self.processed_dir)\n",
    "                if os.path.isdir(os.path.join(self.processed_dir, d))]\n",
    "\n",
    "        if not pdfs:\n",
    "            logger.error(\"No se encontraron PDFs procesados\")\n",
    "            return\n",
    "\n",
    "        self.pdfs = pdfs\n",
    "        self.current_pdf_index = 0\n",
    "        self.current_pdf = self.pdfs[self.current_pdf_index]\n",
    "        self.current_page = 0\n",
    "        self.show_current_page()\n",
    "        self.window.mainloop()\n",
    "\n",
    "    def mark_first_page(self):\n",
    "        \"\"\"Marca la página actual como primera página\"\"\"\n",
    "        if self.current_pdf not in self.labels:\n",
    "            self.labels[self.current_pdf] = {\n",
    "                \"target_pages\": [], \"total_pages\": 0}\n",
    "\n",
    "        if self.current_page not in self.labels[self.current_pdf][\"target_pages\"]:\n",
    "            self.labels[self.current_pdf][\"target_pages\"].append(\n",
    "                self.current_page)\n",
    "            logger.info(\n",
    "                f\"Página {self.current_page} marcada como primera página\")\n",
    "\n",
    "        self.next_page()\n",
    "\n",
    "    def next_page(self):\n",
    "        \"\"\"Pasa a la siguiente página\"\"\"\n",
    "        self.current_page += 1\n",
    "        pages = os.listdir(os.path.join(self.processed_dir, self.current_pdf))\n",
    "\n",
    "        if self.current_page >= len(pages):\n",
    "            self.labels[self.current_pdf][\"total_pages\"] = len(pages)\n",
    "            self.save_labels()\n",
    "            self.load_next_pdf()\n",
    "        else:\n",
    "            self.show_current_page()\n",
    "\n",
    "    def load_next_pdf(self):\n",
    "        \"\"\"Carga el siguiente PDF no etiquetado\"\"\"\n",
    "        self.current_pdf_index += 1\n",
    "\n",
    "        if self.current_pdf_index < len(self.pdfs):\n",
    "            self.current_pdf = self.pdfs[self.current_pdf_index]\n",
    "            self.current_page = 0\n",
    "            self.show_current_page()\n",
    "        else:\n",
    "            logger.info(\"Todos los PDFs han sido etiquetados\")\n",
    "            self.save_and_exit()\n",
    "\n",
    "    def save_labels(self):\n",
    "        \"\"\"Guarda las etiquetas en el archivo\"\"\"\n",
    "        try:\n",
    "            with open(self.labels_file, 'w') as f:\n",
    "                json.dump(self.labels, f, indent=4)\n",
    "            logger.info(\"Etiquetas guardadas correctamente\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error guardando etiquetas: {e}\")\n",
    "\n",
    "    def save_and_exit(self):\n",
    "        \"\"\"Guarda las etiquetas y cierra la aplicación\"\"\"\n",
    "        self.save_labels()\n",
    "        self.window.after(1000, self.window.quit)  # Esperar un segundo antes de cerrar\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    labeler = PDFLabeler()\n",
    "    labeler.start_labeling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configurando Spark...\n",
      "Spark configurado correctamente.\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_0.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_1.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_2.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_3.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_4.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_5.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_6.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_7.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_8.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_9.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_10.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_11.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_12.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_13.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_14.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_15.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_16.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_17.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_18.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_19.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_20.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_21.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_22.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_23.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_24.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_25.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_26.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_27.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_28.png...\n",
      "Preprocesando imagen data/processed\\2-TITULOS-15-DE-NOVIEMBRE-2024-1-30\\page_29.png...\n",
      "DataFrame creado correctamente.\n",
      "Guardando DataFrame en data/processed\\processed_images_with_labels.parquet...\n",
      "DataFrame guardado correctamente.\n",
      "Total de imágenes procesadas: 30\n",
      "\n",
      "Distribución de etiquetas:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    1|    3|\n",
      "|    0|   27|\n",
      "+-----+-----+\n",
      "\n",
      "\n",
      "Verificación de calidad:\n",
      "+-------+--------------------+-------------------+\n",
      "|summary|                path|              label|\n",
      "+-------+--------------------+-------------------+\n",
      "|  count|                  30|                 30|\n",
      "|   mean|                NULL|                0.1|\n",
      "| stddev|                NULL|0.30512857662936466|\n",
      "|    min|data/processed\\2-...|                  0|\n",
      "|    25%|                NULL|                  0|\n",
      "|    50%|                NULL|                  0|\n",
      "|    75%|                NULL|                  0|\n",
      "|    max|data/processed\\2-...|                  1|\n",
      "+-------+--------------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Paso 4: Crear un DataFrame de Spark con las Imágenes y Etiquetas\n",
    "import os\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Configurar Spark\n",
    "print(\"Configurando Spark...\")\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ImageProcessing\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "print(\"Spark configurado correctamente.\")\n",
    "\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_JSON = 'data/labels.json'\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    \"\"\"Preprocesa una imagen para el modelo\"\"\"\n",
    "    try:\n",
    "        print(f\"Preprocesando imagen {image_path}...\")\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized.flatten().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error preprocesando imagen {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Leer etiquetas desde el archivo JSON\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "# Crear DataFrame de imágenes y etiquetas\n",
    "image_data = []\n",
    "for pdf_name, pdf_info in labels.items():\n",
    "    for page_num in range(pdf_info[\"total_pages\"]):\n",
    "        image_path = os.path.join(PROCESSED_DIR, pdf_name, f'page_{page_num}.png')\n",
    "        label = 1 if page_num in pdf_info[\"target_pages\"] else 0\n",
    "        features = preprocess_image(image_path)\n",
    "        if features is not None:\n",
    "            image_data.append((image_path, label, features))\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"features\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(image_data, schema)\n",
    "print(\"DataFrame creado correctamente.\")\n",
    "\n",
    "# Guardar DataFrame procesado\n",
    "output_path = os.path.join(PROCESSED_DIR, \"processed_images_with_labels.parquet\")\n",
    "print(f\"Guardando DataFrame en {output_path}...\")\n",
    "df.write.mode(\"overwrite\").parquet(output_path)\n",
    "print(\"DataFrame guardado correctamente.\")\n",
    "\n",
    "# Mostrar estadísticas del procesamiento\n",
    "print(f\"Total de imágenes procesadas: {df.count()}\")\n",
    "print(\"\\nDistribución de etiquetas:\")\n",
    "df.groupBy(\"label\").count().show()\n",
    "\n",
    "# Verificar valores nulos o problemas\n",
    "print(\"\\nVerificación de calidad:\")\n",
    "df.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Métricas del modelo:\n",
      "Accuracy: 0.6000\n",
      "\n",
      "Estadísticas del procesamiento:\n",
      "Total imágenes: 30\n",
      "\n",
      "Distribución de clases:\n",
      "+-----+-----+\n",
      "|label|count|\n",
      "+-----+-----+\n",
      "|    0|   27|\n",
      "|    1|    3|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configuración de rutas\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_JSON = 'data/labels.json'\n",
    "MODEL_DIR = 'data/models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DocumentClassifier\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def preprocess_image(image_path, target_size=(224, 224)):\n",
    "    try:\n",
    "        image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "        if image is None:\n",
    "            raise ValueError(f\"No se pudo cargar la imagen: {image_path}\")\n",
    "        resized = cv2.resize(image, target_size)\n",
    "        normalized = resized.astype(np.float32) / 255.0\n",
    "        return normalized.flatten().tolist()\n",
    "    except Exception as e:\n",
    "        print(f\"Error procesando {image_path}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Cargar y procesar datos\n",
    "with open(LABELS_JSON, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "\n",
    "image_data = []\n",
    "for pdf_name, pdf_info in labels.items():\n",
    "    for page_num in range(pdf_info[\"total_pages\"]):\n",
    "        image_path = os.path.join(PROCESSED_DIR, pdf_name, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            label = 1 if page_num in pdf_info[\"target_pages\"] else 0\n",
    "            features = preprocess_image(image_path)\n",
    "            if features is not None:\n",
    "                image_data.append((image_path, label, features))\n",
    "\n",
    "# Crear DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"path\", StringType(), False),\n",
    "    StructField(\"label\", IntegerType(), False),\n",
    "    StructField(\"features\", ArrayType(FloatType()), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "# Convertir features a vector\n",
    "array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "# Escalar características\n",
    "scaler = StandardScaler(inputCol=\"features_vector\", \n",
    "                       outputCol=\"scaled_features\",\n",
    "                       withStd=True,\n",
    "                       withMean=True)\n",
    "scaler_model = scaler.fit(df)\n",
    "df_scaled = scaler_model.transform(df)\n",
    "\n",
    "# Preparar datos para entrenamiento\n",
    "final_df = df_scaled.select(\"label\", \"scaled_features\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Entrenar modelo\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaled_features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=20,\n",
    "    regParam=0.1,\n",
    "    elasticNetParam=0.8\n",
    ")\n",
    "\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Evaluar modelo\n",
    "predictions = lr_model.transform(test_df)\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "# Métricas\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"\\nMétricas del modelo:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Guardar modelo y datos procesados\n",
    "lr_model.save(os.path.join(MODEL_DIR, \"logistic_regression_model\"))\n",
    "df_scaled.write.mode(\"overwrite\").parquet(os.path.join(PROCESSED_DIR, \"processed_scaled_data.parquet\"))\n",
    "\n",
    "# Mostrar estadísticas\n",
    "print(\"\\nEstadísticas del procesamiento:\")\n",
    "print(f\"Total imágenes: {df.count()}\")\n",
    "print(\"\\nDistribución de clases:\")\n",
    "df.groupBy(\"label\").count().show()\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\spark-3.5.4-bin-hadoop3\n"
     ]
    }
   ],
   "source": [
    "# echo $SPARK_HOME  # En Linux/Mac\n",
    "!echo %SPARK_HOME% \n",
    "# En Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -c \"import numpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\steev\\documents\\airanfranco\\tributai  ia\\document_classifier\\.venv\\lib\\site-packages (2.0.2)\n",
      "2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
