{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/02/04 23:58:52 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 122, in <module>\n",
      "    predict_first_pages(pdf_dir, model_path, scaler_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 77, in predict_first_pages\n",
      "    lr_model = LogisticRegressionModel.load(model_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 369, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 318, in load\n",
      "    java_obj = self._jread.load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o35.load.\n",
      ": java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\n",
      "\n",
      "\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
      "\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n",
      "CORRECTO: el proceso con PID 16024 (proceso secundario de PID 9292)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 9292 (proceso secundario de PID 14116)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 14116 (proceso secundario de PID 18544)\n",
      "ha sido terminado.\n"
     ]
    }
   ],
   "source": [
    "!python run.py predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HDFS_HOME not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"C:\\Program\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\winutils\n",
      "%HADOOP_HDFS_HOME%\n",
      "C:\\Program Files\\Java\\jdk-21\n",
      "HADOOP_HDFS_HOME not found\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\"C:\\Program\" no se reconoce como un comando interno o externo,\n",
      "programa o archivo por lotes ejecutable.\n"
     ]
    }
   ],
   "source": [
    "# Verificar la versión de Spark\n",
    "# !spark-submit --version\n",
    "\n",
    "\n",
    "# Verificar la versión de Hadoop\n",
    "!hadoop version\n",
    "\n",
    "# Verificar la existencia de winutils.exe\n",
    "# !dir \"C:\\Program Files\\winutils\\bin\\winutils.exe\"\n",
    "\n",
    "# Verificar la existencia de hadoop.dll\n",
    "# !dir \"C:\\Program Files\\winutils\\bin\\hadoop.dll\"\n",
    "\n",
    "!set \"HADOOP_HOME=C:\\Program Files\\winutils\"\n",
    "!set \"HADOOP_HDFS_HOME=C:\\Program Files\\winutils\"\n",
    "!set \"JAVA_HOME=C:\\Program Files\\Java\\jdk-21\"\n",
    "!set \"PATH=%PATH%;%HADOOP_HOME%\\bin;%JAVA_HOME%\\bin\"\n",
    "\n",
    "!echo %HADOOP_HOME%\n",
    "!echo %HADOOP_HDFS_HOME%\n",
    "!echo %JAVA_HOME%\n",
    "\n",
    "!hadoop version\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: HADOOP_HDFS_HOME no está configurado.\n",
      "Faltan algunas bibliotecas necesarias de Hadoop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ctypes\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HDFS_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HDFS_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "if check_hadoop_libraries():\n",
    "    print(\"Todas las bibliotecas necesarias de Hadoop están presentes.\")\n",
    "else:\n",
    "    print(\"Faltan algunas bibliotecas necesarias de Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ctypes\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "if check_hadoop_libraries():\n",
    "    print(\"Todas las bibliotecas necesarias de Hadoop están presentes.\")\n",
    "else:\n",
    "    print(\"Faltan algunas bibliotecas necesarias de Hadoop.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
