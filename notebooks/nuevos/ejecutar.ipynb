{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n",
      "CORRECTO: el proceso con PID 1188 (proceso secundario de PID 10364)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 10364 (proceso secundario de PID 13540)\n",
      "ha sido terminado.\n",
      "CORRECTO: el proceso con PID 13540 (proceso secundario de PID 16088)\n",
      "ha sido terminado.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "Traceback (most recent call last):\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 122, in <module>\n",
      "    predict_first_pages(pdf_dir, model_path, scaler_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\notebooks\\nuevos\\scripts\\predict_first_pages.py\", line 77, in predict_first_pages\n",
      "    lr_model = LogisticRegressionModel.load(model_path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 369, in load\n",
      "    return cls.read().load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\util.py\", line 318, in load\n",
      "    java_obj = self._jread.load(path)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"e:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o35.load.\n",
      ": java.lang.UnsatisfiedLinkError: 'org.apache.hadoop.io.nativeio.NativeIO$POSIX$Stat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(java.lang.String)'\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)\n",
      "\n",
      "\tat org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:608)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:934)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:848)\n",
      "\n",
      "\tat org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:816)\n",
      "\n",
      "\tat org.apache.hadoop.fs.LocatedFileStatus.<init>(LocatedFileStatus.java:52)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2199)\n",
      "\n",
      "\tat org.apache.hadoop.fs.FileSystem$4.next(FileSystem.java:2179)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n",
      "\n",
      "\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n",
      "\n",
      "\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:294)\n",
      "\n",
      "\tat scala.Option.getOrElse(Option.scala:189)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:290)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$take$1(RDD.scala:1471)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.take(RDD.scala:1465)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$first$1(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\n",
      "\tat org.apache.spark.rdd.RDD.first(RDD.scala:1506)\n",
      "\n",
      "\tat org.apache.spark.ml.util.DefaultParamsReader$.loadMetadata(ReadWrite.scala:587)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1326)\n",
      "\n",
      "\tat org.apache.spark.ml.classification.LogisticRegressionModel$LogisticRegressionModelReader.load(LogisticRegression.scala:1320)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\n",
      "\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\n",
      "\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python run.py predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hadoop 3.3.6\n",
      "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
      "Compiled by ubuntu on 2023-06-18T08:22Z\n",
      "Compiled on platform linux-x86_64\n",
      "Compiled with protoc 3.7.1\n",
      "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
      "This command was run using /C:/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n",
      "C:\\hadoop\n",
      "%HADOOP_HDFS_HOME%\n",
      "C:\\Program Files\\Java\\jdk-21\n",
      "Hadoop 3.3.6\n",
      "Source code repository https://github.com/apache/hadoop.git -r 1be78238728da9266a4f88195058f08fd012bf9c\n",
      "Compiled by ubuntu on 2023-06-18T08:22Z\n",
      "Compiled on platform linux-x86_64\n",
      "Compiled with protoc 3.7.1\n",
      "From source with checksum 5652179ad55f76cb287d9c633bb53bbd\n",
      "This command was run using /C:/hadoop/share/hadoop/common/hadoop-common-3.3.6.jar\n"
     ]
    }
   ],
   "source": [
    "# Verificar la versión de Spark\n",
    "# !spark-submit --version\n",
    "\n",
    "\n",
    "# Verificar la versión de Hadoop\n",
    "!hadoop version\n",
    "\n",
    "# Verificar la existencia de winutils.exe\n",
    "# !dir \"C:\\Program Files\\winutils\\bin\\winutils.exe\"\n",
    "\n",
    "# Verificar la existencia de hadoop.dll\n",
    "# !dir \"C:\\Program Files\\winutils\\bin\\hadoop.dll\"\n",
    "\n",
    "!set \"HADOOP_HOME=C:\\Program Files\\winutils\"\n",
    "!set \"HADOOP_HDFS_HOME=C:\\Program Files\\winutils\"\n",
    "!set \"JAVA_HOME=C:\\Program Files\\Java\\jdk-21\"\n",
    "!set \"PATH=%PATH%;%HADOOP_HOME%\\bin;%JAVA_HOME%\\bin\"\n",
    "\n",
    "!echo %HADOOP_HOME%\n",
    "!echo %HADOOP_HDFS_HOME%\n",
    "!echo %JAVA_HOME%\n",
    "\n",
    "!hadoop version\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: HADOOP_HDFS_HOME no está configurado.\n",
      "Faltan algunas bibliotecas necesarias de Hadoop.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ctypes\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HDFS_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HDFS_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "if check_hadoop_libraries():\n",
    "    print(\"Todas las bibliotecas necesarias de Hadoop están presentes.\")\n",
    "else:\n",
    "    print(\"Faltan algunas bibliotecas necesarias de Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Todas las bibliotecas necesarias de Hadoop están presentes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import ctypes\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "if check_hadoop_libraries():\n",
    "    print(\"Todas las bibliotecas necesarias de Hadoop están presentes.\")\n",
    "else:\n",
    "    print(\"Faltan algunas bibliotecas necesarias de Hadoop.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HADOOP_HOME: C:\\Program Files\\winutils\n",
      "HADOOP_HDFS_HOME: C:\\Program Files\\winutils\n",
      "JAVA_HOME: C:\\Program Files\\Java\\jdk-21\n",
      "Error: Hadoop no está instalado o no está en el PATH.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import subprocess\n",
    "\n",
    "# Configurar las variables de entorno\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['HADOOP_HDFS_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "# Verificar la configuración de las variables de entorno\n",
    "print(\"HADOOP_HOME:\", os.environ['HADOOP_HOME'])\n",
    "print(\"HADOOP_HDFS_HOME:\", os.environ['HADOOP_HDFS_HOME'])\n",
    "print(\"JAVA_HOME:\", os.environ['JAVA_HOME'])\n",
    "\n",
    "# Verificar la versión de Hadoop\n",
    "try:\n",
    "    result = subprocess.run(['hadoop', 'version'], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"Hadoop Version:\")\n",
    "        print(result.stdout)\n",
    "    else:\n",
    "        print(\"Error al obtener la versión de Hadoop:\")\n",
    "        print(result.stderr)\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Hadoop no está instalado o no está en el PATH.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Páginas predichas como primeras páginas:\n",
      "data/pdf_pages\\page_0.png\n",
      "data/pdf_pages\\page_10.png\n",
      "data/pdf_pages\\page_20.png\n",
      "data/pdf_pages\\page_30.png\n",
      "data/pdf_pages\\page_40.png\n",
      "data/pdf_pages\\page_50.png\n",
      "data/pdf_pages\\page_60.png\n",
      "data/pdf_pages\\page_70.png\n",
      "data/pdf_pages\\page_80.png\n",
      "data/pdf_pages\\page_90.png\n",
      "data/pdf_pages\\page_100.png\n",
      "data/pdf_pages\\page_110.png\n",
      "data/pdf_pages\\page_114.png\n",
      "data/pdf_pages\\page_116.png\n",
      "data/pdf_pages\\page_124.png\n",
      "data/pdf_pages\\page_134.png\n",
      "data/pdf_pages\\page_140.png\n",
      "data/pdf_pages\\page_146.png\n",
      "data/pdf_pages\\page_152.png\n",
      "data/pdf_pages\\page_162.png\n",
      "data/pdf_pages\\page_170.png\n",
      "data/pdf_pages\\page_178.png\n",
      "data/pdf_pages\\page_184.png\n",
      "data/pdf_pages\\page_190.png\n",
      "data/pdf_pages\\page_198.png\n",
      "data/pdf_pages\\page_206.png\n",
      "data/pdf_pages\\page_214.png\n",
      "data/pdf_pages\\page_222.png\n",
      "data/pdf_pages\\page_244.png\n",
      "data/pdf_pages\\page_254.png\n",
      "data/pdf_pages\\page_264.png\n",
      "data/pdf_pages\\page_274.png\n",
      "data/pdf_pages\\page_284.png\n",
      "data/pdf_pages\\page_290.png\n",
      "data/pdf_pages\\page_296.png\n",
      "data/pdf_pages\\page_306.png\n",
      "data/pdf_pages\\page_312.png\n",
      "data/pdf_pages\\page_322.png\n",
      "data/pdf_pages\\page_332.png\n",
      "data/pdf_pages\\page_346.png\n",
      "data/pdf_pages\\page_356.png\n",
      "data/pdf_pages\\page_360.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Verificar las bibliotecas necesarias de Hadoop antes de ejecutar el script\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configurar las bibliotecas nativas de Hadoop\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path):\n",
    "    # Verificar si el modelo y el escalador existen\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    # Cargar el modelo y el escalador\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    # Procesar las páginas del PDF\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            # Leer la imagen y convertirla a un vector\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                image_data.append((image_path, features))\n",
    "\n",
    "    # Crear DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    # Convertir features a vector\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    # Escalar características\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    # Filtrar las primeras páginas predichas\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_dir = 'data/pdf_pages'\n",
    "model_path = 'data/models/logistic_regression_model'\n",
    "scaler_path = 'data/processed/scaler_model'\n",
    "predict_first_pages(pdf_dir, model_path, scaler_path)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
