{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del Modelo con PySpark\n",
    "\n",
    "Este notebook implementa el entrenamiento del modelo usando PySpark y TensorFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Iniciar Spark con más memoria para procesamiento de imágenes\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ModelTraining\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar paths\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "LABELS_FILE = 'data/labels.json'\n",
    "MODEL_DIR = 'data/models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar datos procesados\n",
    "print(\"Cargando datos procesados...\")\n",
    "df = spark.read.parquet(os.path.join(PROCESSED_DIR, \"processed_pages.parquet\"))\n",
    "print(f\"Total de registros cargados: {df.count()}\")\n",
    "\n",
    "# Cargar etiquetas\n",
    "print(\"\\nCargando archivo de etiquetas...\")\n",
    "with open(LABELS_FILE, 'r') as f:\n",
    "    labels = json.load(f)\n",
    "print(f\"Etiquetas cargadas para {len(labels)} documentos\")\n",
    "\n",
    "# Función para determinar si una página es primera página\n",
    "def is_first_page(pdf_name, page_number):\n",
    "    if pdf_name in labels:\n",
    "        return 1 if page_number in labels[pdf_name][\"target_pages\"] else 0\n",
    "    return 0\n",
    "\n",
    "# Registrar UDF\n",
    "is_first_page_udf = spark.udf.register(\"is_first_page\", is_first_page)\n",
    "\n",
    "# Añadir etiquetas al DataFrame\n",
    "print(\"\\nAsignando etiquetas a los datos...\")\n",
    "df = df.withColumn(\"label\", \n",
    "    is_first_page_udf(col(\"pdf_name\"), col(\"page_number\")))\n",
    "\n",
    "# Mostrar distribución de etiquetas\n",
    "print(\"\\nDistribución de etiquetas:\")\n",
    "df.groupBy(\"label\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para entrenamiento\n",
    "print(\"Preparando datos para entrenamiento...\")\n",
    "\n",
    "# Convertir a numpy arrays para TensorFlow\n",
    "train_data = df.select(\"features\", \"label\").toPandas()\n",
    "X = np.array([np.array(x) for x in train_data[\"features\"]])\n",
    "y = train_data[\"label\"].values\n",
    "\n",
    "# Dividir en entrenamiento y validación\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, \n",
    "    test_size=0.2, \n",
    "    random_state=42, \n",
    "    stratify=y  # Mantener proporción de clases\n",
    ")\n",
    "\n",
    "print(f\"\\nDimensiones de los datos:\")\n",
    "print(f\"X_train: {X_train.shape}\")\n",
    "print(f\"X_val: {X_val.shape}\")\n",
    "print(f\"y_train: {np.bincount(y_train)}\")\n",
    "print(f\"y_val: {np.bincount(y_val)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(input_shape=(224*224,)):\n",
    "    \"\"\"Crea y retorna el modelo CNN\"\"\"\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=input_shape),\n",
    "        tf.keras.layers.Reshape((224, 224, 1)),\n",
    "        \n",
    "        # Primer bloque convolucional\n",
    "        tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Segundo bloque convolucional\n",
    "        tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Tercer bloque convolucional\n",
    "        tf.keras.layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "        tf.keras.layers.BatchNormalization(),\n",
    "        tf.keras.layers.MaxPooling2D((2, 2)),\n",
    "        \n",
    "        # Capas densas\n",
    "        tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.Dense(512, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.5),\n",
    "        tf.keras.layers.Dense(256, activation='relu'),\n",
    "        tf.keras.layers.Dropout(0.3),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "    \n",
    "    # Compilar modelo\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=[\n",
    "            'accuracy',\n",
    "            tf.keras.metrics.Precision(),\n",
    "            tf.keras.metrics.Recall(),\n",
    "            tf.keras.metrics.AUC()\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Crear y mostrar resumen del modelo\n",
    "print(\"Creando modelo...\")\n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurar callbacks\n",
    "checkpoint_path = os.path.join(MODEL_DIR, \"checkpoints\")\n",
    "os.makedirs(checkpoint_path, exist_ok=True)\n",
    "\n",
    "callbacks = [\n",
    "    # Guardar mejor modelo\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=os.path.join(checkpoint_path, \"model_{epoch:02d}-{val_loss:.2f}.h5\"),\n",
    "        save_best_only=True,\n",
    "        monitor='val_loss'\n",
    "    ),\n",
    "    # Detener si no hay mejora\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        patience=5,\n",
    "        restore_best_weights=True,\n",
    "        monitor='val_loss'\n",
    "    ),\n",
    "    # Reducir learning rate si no hay mejora\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.2,\n",
    "        patience=3,\n",
    "        min_lr=1e-6\n",
    "    ),\n",
    "    # TensorBoard logging\n",
    "    tf.keras.callbacks.TensorBoard(\n",
    "        log_dir=os.path.join(MODEL_DIR, 'logs', datetime.now().strftime(\"%Y%m%d-%H%M%S\")),\n",
    "        histogram_freq=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Entrenar modelo\n",
    "print(\"Iniciando entrenamiento...\")\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=20,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    class_weight={  # Manejar desbalance de clases\n",
    "        0: 1.,\n",
    "        1: np.sum(y_train == 0) / np.sum(y_train == 1)\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar modelo final\n",
    "print(\"Guardando modelo y resultados...\")\n",
    "model_path = os.path.join(MODEL_DIR, \"final_model.h5\")\n",
    "model.save(model_path)\n",
    "print(f\"Modelo guardado en: {model_path}\")\n",
    "\n",
    "# Guardar historia de entrenamiento\n",
    "history_path = os.path.join(MODEL_DIR, \"training_history.pkl\")\n",
    "with open(history_path, \"wb\") as f:\n",
    "    pickle.dump(history.history, f)\n",
    "print(f\"Historia de entrenamiento guardada en: {history_path}\")\n",
    "\n",
    "# Mostrar resultados finales\n",
    "print(\"\\nResultados finales del entrenamiento:\")\n",
    "for metric in history.history:\n",
    "    if not metric.startswith('val_'):\n",
    "        print(f\"{metric}: {history.history[metric][-1]:.4f} \"\n",
    "              f\"(validación: {history.history['val_'+metric][-1]:.4f})\")\n",
    "\n",
    "# Limpiar sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}