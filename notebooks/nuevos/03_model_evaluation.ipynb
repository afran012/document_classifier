{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluación del Modelo\n",
    "\n",
    "Este notebook evalúa el rendimiento del modelo entrenado utilizando PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from pyspark.sql import SparkSession\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "# Iniciar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ModelEvaluation\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Configurar paths\n",
    "MODEL_DIR = 'data/models'\n",
    "PROCESSED_DIR = 'data/processed'\n",
    "EVAL_DIR = os.path.join(MODEL_DIR, 'evaluation')\n",
    "os.makedirs(EVAL_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar modelo\n",
    "print(\"Cargando modelo...\")\n",
    "model = tf.keras.models.load_model(os.path.join(MODEL_DIR, \"final_model.h5\"))\n",
    "\n",
    "# Cargar historia de entrenamiento\n",
    "print(\"Cargando historial de entrenamiento...\")\n",
    "with open(os.path.join(MODEL_DIR, \"training_history.pkl\"), \"rb\") as f:\n",
    "    history = pickle.load(f)\n",
    "\n",
    "# Cargar datos de validación\n",
    "print(\"Cargando datos de validación...\")\n",
    "df_val = spark.read.parquet(os.path.join(PROCESSED_DIR, \"processed_pages.parquet\"))\n",
    "val_data = df_val.toPandas()\n",
    "\n",
    "# Preparar datos\n",
    "X_val = np.array([np.array(x) for x in val_data[\"features\"]])\n",
    "y_val = val_data[\"label\"].values\n",
    "\n",
    "# Realizar predicciones\n",
    "print(\"Realizando predicciones...\")\n",
    "y_pred = model.predict(X_val)\n",
    "y_pred_classes = (y_pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(history):\n",
    "    \"\"\"Visualiza el historial de entrenamiento\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['loss'], label='Training Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(EVAL_DIR, 'training_history.png'))\n",
    "    plt.show()\n",
    "\n",
    "print(\"Visualizando historial de entrenamiento...\")\n",
    "plot_training_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    \"\"\"Visualiza la matriz de confusión\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=['No Primera', 'Primera'],\n",
    "                yticklabels=['No Primera', 'Primera'])\n",
    "    plt.title('Matriz de Confusión')\n",
    "    plt.ylabel('Etiqueta Verdadera')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.savefig(os.path.join(EVAL_DIR, 'confusion_matrix.png'))\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nGenerando matriz de confusión...\")\n",
    "plot_confusion_matrix(y_val, y_pred_classes)\n",
    "\n",
    "# Imprimir reporte de clasificación\n",
    "print(\"\\nReporte de Clasificación:\")\n",
    "print(classification_report(y_val, y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(y_true, y_pred_prob):\n",
    "    \"\"\"Visualiza la curva ROC\"\"\"\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2,\n",
    "             label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('Tasa de Falsos Positivos')\n",
    "    plt.ylabel('Tasa de Verdaderos Positivos')\n",
    "    plt.title('Curva ROC')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.savefig(os.path.join(EVAL_DIR, 'roc_curve.png'))\n",
    "    plt.show()\n",
    "    return roc_auc\n",
    "\n",
    "print(\"\\nGenerando curva ROC...\")\n",
    "roc_auc = plot_roc_curve(y_val, y_pred)\n",
    "print(f\"AUC-ROC: {roc_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_errors(X_val, y_val, y_pred, y_pred_classes, val_data, num_examples=5):\n",
    "    \"\"\"Analiza los errores del modelo\"\"\"\n",
    "    # Encontrar errores\n",
    "    errors = np.where(y_val != y_pred_classes)[0]\n",
    "    \n",
    "    # Crear DataFrame con información de errores\n",
    "    error_info = pd.DataFrame({\n",
    "        'True_Label': y_val[errors],\n",
    "        'Predicted_Label': y_pred_classes[errors],\n",
    "        'Confidence': y_pred[errors],\n",
    "        'PDF_Name': val_data.iloc[errors]['pdf_name'],\n",
    "        'Page_Number': val_data.iloc[errors]['page_number']\n",
    "    })\n",
    "    \n",
    "    print(\"\\nAnálisis de Errores:\")\n",
    "    print(f\"Total de errores: {len(errors)}\")\n",
    "    print(f\"Tasa de error: {len(errors)/len(y_val)*100:.2f}%\")\n",
    "    \n",
    "    # Mostrar algunos ejemplos de errores\n",
    "    print(\"\\nEjemplos de predicciones incorrectas:\")\n",
    "    print(error_info.head(num_examples))\n",
    "    \n",
    "    # Guardar información de errores\n",
    "    error_info.to_csv(os.path.join(EVAL_DIR, 'error_analysis.csv'), index=False)\n",
    "    return error_info\n",
    "\n",
    "print(\"\\nRealizando análisis de errores...\")\n",
    "error_analysis = analyze_errors(X_val, y_val, y_pred, y_pred_classes, val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generar reporte final\n",
    "report = {\n",
    "    'model_performance': {\n",
    "        'accuracy': (y_val == y_pred_classes).mean(),\n",
    "        'auc_roc': roc_auc,\n",
    "        'total_samples': len(y_val),\n",
    "        'error_rate': len(error_analysis) / len(y_val)\n",
    "    },\n",
    "    'class_distribution': {\n",
    "        'actual': dict(zip(*np.unique(y_val, return_counts=True))),\n",
    "        'predicted': dict(zip(*np.unique(y_pred_classes, return_counts=True)))\n",
    "    }\n",
    "}\n",
    "\n",
    "# Guardar reporte\n",
    "with open(os.path.join(EVAL_DIR, 'evaluation_report.json'), 'w') as f:\n",
    "    json.dump(report, f, indent=4)\n",
    "\n",
    "print(\"\\nReporte Final:\")\n",
    "for metric, value in report['model_performance'].items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Limpiar sesión de Spark\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}