{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Páginas predichas como primeras páginas:\n",
      "data/pdf_pages\\page_0.png\n",
      "data/pdf_pages\\page_10.png\n",
      "data/pdf_pages\\page_20.png\n",
      "data/pdf_pages\\page_30.png\n",
      "data/pdf_pages\\page_40.png\n",
      "data/pdf_pages\\page_50.png\n",
      "data/pdf_pages\\page_60.png\n",
      "data/pdf_pages\\page_70.png\n",
      "data/pdf_pages\\page_80.png\n",
      "data/pdf_pages\\page_90.png\n",
      "data/pdf_pages\\page_100.png\n",
      "data/pdf_pages\\page_110.png\n",
      "data/pdf_pages\\page_114.png\n",
      "data/pdf_pages\\page_116.png\n",
      "data/pdf_pages\\page_124.png\n",
      "data/pdf_pages\\page_134.png\n",
      "data/pdf_pages\\page_140.png\n",
      "data/pdf_pages\\page_146.png\n",
      "data/pdf_pages\\page_152.png\n",
      "data/pdf_pages\\page_162.png\n",
      "data/pdf_pages\\page_170.png\n",
      "data/pdf_pages\\page_178.png\n",
      "data/pdf_pages\\page_184.png\n",
      "data/pdf_pages\\page_190.png\n",
      "data/pdf_pages\\page_198.png\n",
      "data/pdf_pages\\page_206.png\n",
      "data/pdf_pages\\page_214.png\n",
      "data/pdf_pages\\page_222.png\n",
      "data/pdf_pages\\page_244.png\n",
      "data/pdf_pages\\page_254.png\n",
      "data/pdf_pages\\page_264.png\n",
      "data/pdf_pages\\page_274.png\n",
      "data/pdf_pages\\page_284.png\n",
      "data/pdf_pages\\page_290.png\n",
      "data/pdf_pages\\page_296.png\n",
      "data/pdf_pages\\page_306.png\n",
      "data/pdf_pages\\page_312.png\n",
      "data/pdf_pages\\page_322.png\n",
      "data/pdf_pages\\page_332.png\n",
      "data/pdf_pages\\page_346.png\n",
      "data/pdf_pages\\page_356.png\n",
      "data/pdf_pages\\page_360.png\n",
      "Precisión: 0.9762\n",
      "Recall: 1.0000\n",
      "F1-score: 0.9880\n"
     ]
    }
   ],
   "source": [
    "# Paso 1: Guardar los Resultados de las Predicciones\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import cv2\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Verificar las bibliotecas necesarias de Hadoop antes de ejecutar el script\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configurar las bibliotecas nativas de Hadoop\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def calculate_metrics(predicted_pages, target_pages):\n",
    "    true_positives = len(set(predicted_pages) & set(target_pages))\n",
    "    false_positives = len(set(predicted_pages) - set(target_pages))\n",
    "    false_negatives = len(set(target_pages) - set(predicted_pages))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path, target_pages, output_file):\n",
    "    # Verificar si el modelo y el escalador existen\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    # Cargar el modelo y el escalador\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    # Procesar las páginas del PDF\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            # Leer la imagen y convertirla a un vector\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                image_data.append((image_path, features))\n",
    "\n",
    "    # Crear DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    # Convertir features a vector\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    # Escalar características\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    # Filtrar las primeras páginas predichas\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    # Guardar resultados en un archivo de texto\n",
    "    with open(output_file, 'w') as f:\n",
    "        for row in first_pages:\n",
    "            f.write(row[\"path\"] + '\\n')\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "    # Calcular precisión\n",
    "    predicted_pages = [int(row[\"path\"].split('_')[-1].split('.')[0]) for row in first_pages]\n",
    "    precision, recall, f1_score = calculate_metrics(predicted_pages, target_pages)\n",
    "    print(f\"Precisión: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1_score:.4f}\")\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_dir = 'data/pdf_pages'\n",
    "model_path = 'data/models/logistic_regression_model'\n",
    "scaler_path = 'data/processed/scaler_model'\n",
    "target_pages = [\n",
    "      0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, 114, 124,\n",
    "      134, 140, 146, 152, 162, 170, 178, 184, 190, 198, 206, 214, 222, 244,\n",
    "      254, 264, 274, 284, 290, 296, 306, 312, 322, 332, 346, 356, 360\n",
    "    ]\n",
    "output_file = 'predicted_first_pages.txt'\n",
    "predict_first_pages(pdf_dir, model_path, scaler_path, target_pages, output_file)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Páginas predichas como primeras páginas:\n",
      "data/pdf_pages\\page_0.png\n",
      "data/pdf_pages\\page_10.png\n",
      "data/pdf_pages\\page_20.png\n",
      "data/pdf_pages\\page_30.png\n",
      "data/pdf_pages\\page_40.png\n",
      "data/pdf_pages\\page_50.png\n",
      "data/pdf_pages\\page_60.png\n",
      "data/pdf_pages\\page_70.png\n",
      "data/pdf_pages\\page_80.png\n",
      "data/pdf_pages\\page_90.png\n",
      "data/pdf_pages\\page_100.png\n",
      "data/pdf_pages\\page_110.png\n",
      "data/pdf_pages\\page_114.png\n",
      "data/pdf_pages\\page_116.png\n",
      "data/pdf_pages\\page_124.png\n",
      "data/pdf_pages\\page_134.png\n",
      "data/pdf_pages\\page_140.png\n",
      "data/pdf_pages\\page_146.png\n",
      "data/pdf_pages\\page_152.png\n",
      "data/pdf_pages\\page_162.png\n",
      "data/pdf_pages\\page_170.png\n",
      "data/pdf_pages\\page_178.png\n",
      "data/pdf_pages\\page_184.png\n",
      "data/pdf_pages\\page_190.png\n",
      "data/pdf_pages\\page_198.png\n",
      "data/pdf_pages\\page_206.png\n",
      "data/pdf_pages\\page_214.png\n",
      "data/pdf_pages\\page_222.png\n",
      "data/pdf_pages\\page_244.png\n",
      "data/pdf_pages\\page_254.png\n",
      "data/pdf_pages\\page_264.png\n",
      "data/pdf_pages\\page_274.png\n",
      "data/pdf_pages\\page_284.png\n",
      "data/pdf_pages\\page_290.png\n",
      "data/pdf_pages\\page_296.png\n",
      "data/pdf_pages\\page_306.png\n",
      "data/pdf_pages\\page_312.png\n",
      "data/pdf_pages\\page_322.png\n",
      "data/pdf_pages\\page_332.png\n",
      "data/pdf_pages\\page_346.png\n",
      "data/pdf_pages\\page_356.png\n",
      "data/pdf_pages\\page_360.png\n",
      "Precisión: 0.9762\n",
      "Recall: 1.0000\n",
      "F1-score: 0.9880\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o204.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6402.0 failed 1 times, most recent failure: Lost task 7.0 in stage 6402.0 (TID 62071) (Loboreapper executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:192)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:181)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:275)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:106)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:126)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:101)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 194\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# Evaluar el modelo\u001b[39;00m\n\u001b[0;32m    193\u001b[0m predictions \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mtransform(test_df)\n\u001b[1;32m--> 194\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mROC AUC: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mroc_auc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# Guardar el mejor modelo\u001b[39;00m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\evaluation.py:111\u001b[0m, in \u001b[0;36mEvaluator.evaluate\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_evaluate(dataset)\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\evaluation.py:148\u001b[0m, in \u001b[0;36mJavaEvaluator._evaluate\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m    147\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o204.evaluate.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 6402.0 failed 1 times, most recent failure: Lost task 7.0 in stage 6402.0 (TID 62071) (Loboreapper executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\r\n\tat org.apache.spark.RangePartitioner$.sketch(Partitioner.scala:320)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:187)\r\n\tat org.apache.spark.RangePartitioner.<init>(Partitioner.scala:167)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.$anonfun$sortByKey$1(OrderedRDDFunctions.scala:64)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.OrderedRDDFunctions.sortByKey(OrderedRDDFunctions.scala:63)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3$lzycompute(BinaryClassificationMetrics.scala:192)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.x$3(BinaryClassificationMetrics.scala:181)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions$lzycompute(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.confusions(BinaryClassificationMetrics.scala:183)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.createCurve(BinaryClassificationMetrics.scala:275)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.roc(BinaryClassificationMetrics.scala:106)\r\n\tat org.apache.spark.mllib.evaluation.BinaryClassificationMetrics.areaUnderROC(BinaryClassificationMetrics.scala:126)\r\n\tat org.apache.spark.ml.evaluation.BinaryClassificationEvaluator.evaluate(BinaryClassificationEvaluator.scala:101)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de winutils.exe\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Verificar la existencia de hadoop.dll\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    # Intentar cargar hadoop.dll\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "# Verificar las bibliotecas necesarias de Hadoop antes de ejecutar el script\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "# Configuración del entorno\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "\n",
    "# Configurar las bibliotecas nativas de Hadoop\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "def load_target_pages(labels_path, document_name):\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    return labels[document_name][\"target_pages\"]\n",
    "\n",
    "def calculate_metrics(predicted_pages, target_pages):\n",
    "    true_positives = len(set(predicted_pages) & set(target_pages))\n",
    "    false_positives = len(set(predicted_pages) - set(target_pages))\n",
    "    false_negatives = len(set(target_pages) - set(predicted_pages))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path, target_pages, output_file):\n",
    "    # Verificar si el modelo y el escalador existen\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    # Cargar el modelo y el escalador\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    # Procesar las páginas del PDF\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            # Leer la imagen y convertirla a un vector\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                label = 1 if page_num in target_pages else 0\n",
    "                image_data.append((image_path, label, features))\n",
    "\n",
    "    # Crear DataFrame\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"label\", IntegerType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    # Convertir features a vector\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    # Escalar características\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    # Filtrar las primeras páginas predichas\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    # Guardar resultados en un archivo de texto\n",
    "    with open(output_file, 'w') as f:\n",
    "        for row in first_pages:\n",
    "            f.write(row[\"path\"] + '\\n')\n",
    "\n",
    "    # Mostrar resultados\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "    # Calcular precisión\n",
    "    predicted_pages = [int(row[\"path\"].split('_')[-1].split('.')[0]) for row in first_pages]\n",
    "    precision, recall, f1_score = calculate_metrics(predicted_pages, target_pages)\n",
    "    print(f\"Precisión: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1_score:.4f}\")\n",
    "\n",
    "    return first_pages, df_scaled\n",
    "\n",
    "# Ejemplo de uso\n",
    "pdf_dir = 'data/pdf_pages'\n",
    "model_path = 'data/models/logistic_regression_model'\n",
    "scaler_path = 'data/processed/scaler_model'\n",
    "labels_path = 'data/labels.json'\n",
    "document_name = \"2-TITULOS-15-DE-NOVIEMBRE-2024\"\n",
    "target_pages = load_target_pages(labels_path, document_name)\n",
    "output_file = 'predicted_first_pages.txt'\n",
    "first_pages, df_scaled = predict_first_pages(pdf_dir, model_path, scaler_path, target_pages, output_file)\n",
    "\n",
    "# Realizar Tuning del Modelo\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Definir el modelo de regresión logística\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "# Definir el evaluador\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 20, 50]) \\\n",
    "    .build()\n",
    "\n",
    "# Definir el validador cruzado\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Preparar datos para entrenamiento\n",
    "final_df = df_scaled.select(\"label\", \"scaled_features\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Entrenar el modelo con validación cruzada\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "# Evaluar el modelo\n",
    "predictions = cvModel.transform(test_df)\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = cvModel.bestModel\n",
    "best_model.write().overwrite().save(os.path.join('data/models', \"best_logistic_regression_model\"))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 3: Realizar Tuning del Modelo\n",
    "\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Definir el evaluador\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Definir la cuadrícula de hiperparámetros\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 20, 50]) \\\n",
    "    .build()\n",
    "\n",
    "# Definir el validador cruzado\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "# Entrenar el modelo con validación cruzada\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "# Evaluar el modelo\n",
    "predictions = cvModel.transform(test_df)\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "best_model = cvModel.bestModel\n",
    "best_model.write().overwrite().save(os.path.join(MODEL_DIR, \"best_logistic_regression_model\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-3.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Páginas predichas como primeras páginas:\n",
      "data/pdf_pages\\page_0.png\n",
      "data/pdf_pages\\page_10.png\n",
      "data/pdf_pages\\page_20.png\n",
      "data/pdf_pages\\page_30.png\n",
      "data/pdf_pages\\page_40.png\n",
      "data/pdf_pages\\page_50.png\n",
      "data/pdf_pages\\page_60.png\n",
      "data/pdf_pages\\page_70.png\n",
      "data/pdf_pages\\page_80.png\n",
      "data/pdf_pages\\page_90.png\n",
      "data/pdf_pages\\page_100.png\n",
      "data/pdf_pages\\page_110.png\n",
      "data/pdf_pages\\page_114.png\n",
      "data/pdf_pages\\page_116.png\n",
      "data/pdf_pages\\page_124.png\n",
      "data/pdf_pages\\page_134.png\n",
      "data/pdf_pages\\page_140.png\n",
      "data/pdf_pages\\page_146.png\n",
      "data/pdf_pages\\page_152.png\n",
      "data/pdf_pages\\page_162.png\n",
      "data/pdf_pages\\page_170.png\n",
      "data/pdf_pages\\page_178.png\n",
      "data/pdf_pages\\page_184.png\n",
      "data/pdf_pages\\page_190.png\n",
      "data/pdf_pages\\page_198.png\n",
      "data/pdf_pages\\page_206.png\n",
      "data/pdf_pages\\page_214.png\n",
      "data/pdf_pages\\page_222.png\n",
      "data/pdf_pages\\page_244.png\n",
      "data/pdf_pages\\page_254.png\n",
      "data/pdf_pages\\page_264.png\n",
      "data/pdf_pages\\page_274.png\n",
      "data/pdf_pages\\page_284.png\n",
      "data/pdf_pages\\page_290.png\n",
      "data/pdf_pages\\page_296.png\n",
      "data/pdf_pages\\page_306.png\n",
      "data/pdf_pages\\page_312.png\n",
      "data/pdf_pages\\page_322.png\n",
      "data/pdf_pages\\page_332.png\n",
      "data/pdf_pages\\page_346.png\n",
      "data/pdf_pages\\page_356.png\n",
      "data/pdf_pages\\page_360.png\n",
      "Precisión: 0.9762\n",
      "Recall: 1.0000\n",
      "F1-score: 0.9880\n",
      "Segmento 1 guardado en data/output\\segment_1.pdf\n",
      "Segmento 2 guardado en data/output\\segment_2.pdf\n",
      "Segmento 3 guardado en data/output\\segment_3.pdf\n",
      "Segmento 4 guardado en data/output\\segment_4.pdf\n",
      "Segmento 5 guardado en data/output\\segment_5.pdf\n",
      "Segmento 6 guardado en data/output\\segment_6.pdf\n",
      "Segmento 7 guardado en data/output\\segment_7.pdf\n",
      "Segmento 8 guardado en data/output\\segment_8.pdf\n",
      "Segmento 9 guardado en data/output\\segment_9.pdf\n",
      "Segmento 10 guardado en data/output\\segment_10.pdf\n",
      "Segmento 11 guardado en data/output\\segment_11.pdf\n",
      "Segmento 12 guardado en data/output\\segment_12.pdf\n",
      "Segmento 13 guardado en data/output\\segment_13.pdf\n",
      "Segmento 14 guardado en data/output\\segment_14.pdf\n",
      "Segmento 15 guardado en data/output\\segment_15.pdf\n",
      "Segmento 16 guardado en data/output\\segment_16.pdf\n",
      "Segmento 17 guardado en data/output\\segment_17.pdf\n",
      "Segmento 18 guardado en data/output\\segment_18.pdf\n",
      "Segmento 19 guardado en data/output\\segment_19.pdf\n",
      "Segmento 20 guardado en data/output\\segment_20.pdf\n",
      "Segmento 21 guardado en data/output\\segment_21.pdf\n",
      "Segmento 22 guardado en data/output\\segment_22.pdf\n",
      "Segmento 23 guardado en data/output\\segment_23.pdf\n",
      "Segmento 24 guardado en data/output\\segment_24.pdf\n",
      "Segmento 25 guardado en data/output\\segment_25.pdf\n",
      "Segmento 26 guardado en data/output\\segment_26.pdf\n",
      "Segmento 27 guardado en data/output\\segment_27.pdf\n",
      "Segmento 28 guardado en data/output\\segment_28.pdf\n",
      "Segmento 29 guardado en data/output\\segment_29.pdf\n",
      "Segmento 30 guardado en data/output\\segment_30.pdf\n",
      "Segmento 31 guardado en data/output\\segment_31.pdf\n",
      "Segmento 32 guardado en data/output\\segment_32.pdf\n",
      "Segmento 33 guardado en data/output\\segment_33.pdf\n",
      "Segmento 34 guardado en data/output\\segment_34.pdf\n",
      "Segmento 35 guardado en data/output\\segment_35.pdf\n",
      "Segmento 36 guardado en data/output\\segment_36.pdf\n",
      "Segmento 37 guardado en data/output\\segment_37.pdf\n",
      "Segmento 38 guardado en data/output\\segment_38.pdf\n",
      "Segmento 39 guardado en data/output\\segment_39.pdf\n",
      "Segmento 40 guardado en data/output\\segment_40.pdf\n",
      "Segmento 41 guardado en data/output\\segment_41.pdf\n",
      "Segmento 42 guardado en data/output\\segment_42.pdf\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o20503.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 6524.0 failed 1 times, most recent failure: Lost task 13.0 in stage 6524.0 (TID 63201) (Loboreapper executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\r\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\r\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:53)\r\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:47)\r\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:99)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1005)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 195\u001b[0m\n\u001b[0;32m    192\u001b[0m final_df \u001b[38;5;241m=\u001b[39m df_scaled\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscaled_features\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    193\u001b[0m train_df, test_df \u001b[38;5;241m=\u001b[39m final_df\u001b[38;5;241m.\u001b[39mrandomSplit([\u001b[38;5;241m0.8\u001b[39m, \u001b[38;5;241m0.2\u001b[39m], seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m--> 195\u001b[0m cvModel \u001b[38;5;241m=\u001b[39m \u001b[43mcrossval\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m predictions \u001b[38;5;241m=\u001b[39m cvModel\u001b[38;5;241m.\u001b[39mtransform(test_df)\n\u001b[0;32m    198\u001b[0m roc_auc \u001b[38;5;241m=\u001b[39m evaluator\u001b[38;5;241m.\u001b[39mevaluate(predictions)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\base.py:205\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_fit(dataset)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[0;32m    210\u001b[0m     )\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\tuning.py:862\u001b[0m, in \u001b[0;36mCrossValidator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    861\u001b[0m     bestIndex \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmin(metrics)\n\u001b[1;32m--> 862\u001b[0m bestModel \u001b[38;5;241m=\u001b[39m \u001b[43mest\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepm\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbestIndex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(\n\u001b[0;32m    864\u001b[0m     CrossValidatorModel(bestModel, metrics, cast(List[List[Model]], subModels), std_metrics)\n\u001b[0;32m    865\u001b[0m )\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:381\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 381\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    382\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\ml\\wrapper.py:378\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    375\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 378\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32me:\\proyectos\\validarpdf\\document_classifier\\.venv\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o20503.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 6524.0 failed 1 times, most recent failure: Lost task 13.0 in stage 6524.0 (TID 63201) (Loboreapper executor driver): java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2488)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$fold$1(RDD.scala:1202)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.fold(RDD.scala:1196)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$2(RDD.scala:1289)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1256)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$treeAggregate$1(RDD.scala:1242)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\r\n\tat org.apache.spark.rdd.RDD.treeAggregate(RDD.scala:1242)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:61)\r\n\tat org.apache.spark.ml.optim.loss.RDDLossFunction.calculate(RDDLossFunction.scala:47)\r\n\tat breeze.optimize.CachedDiffFunction.calculate(CachedDiffFunction.scala:24)\r\n\tat breeze.optimize.FirstOrderMinimizer.calculateObjective(FirstOrderMinimizer.scala:53)\r\n\tat breeze.optimize.FirstOrderMinimizer.initialState(FirstOrderMinimizer.scala:47)\r\n\tat breeze.optimize.FirstOrderMinimizer.iterations(FirstOrderMinimizer.scala:99)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.trainImpl(LogisticRegression.scala:1005)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.$anonfun$train$1(LogisticRegression.scala:634)\r\n\tat org.apache.spark.ml.util.Instrumentation$.$anonfun$instrumented$1(Instrumentation.scala:191)\r\n\tat scala.util.Try$.apply(Try.scala:213)\r\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:191)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:497)\r\n\tat org.apache.spark.ml.classification.LogisticRegression.train(LogisticRegression.scala:287)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:114)\r\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:78)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:75)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:52)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:1583)\r\nCaused by: java.net.SocketException: Connection reset\r\n\tat java.base/sun.nio.ch.NioSocketImpl.implRead(NioSocketImpl.java:318)\r\n\tat java.base/sun.nio.ch.NioSocketImpl.read(NioSocketImpl.java:346)\r\n\tat java.base/sun.nio.ch.NioSocketImpl$1.read(NioSocketImpl.java:796)\r\n\tat java.base/java.net.Socket$SocketInputStream.read(Socket.java:1099)\r\n\tat java.base/java.io.BufferedInputStream.fill(BufferedInputStream.java:291)\r\n\tat java.base/java.io.BufferedInputStream.read1(BufferedInputStream.java:347)\r\n\tat java.base/java.io.BufferedInputStream.implRead(BufferedInputStream.java:420)\r\n\tat java.base/java.io.BufferedInputStream.read(BufferedInputStream.java:399)\r\n\tat java.base/java.io.DataInputStream.readFully(DataInputStream.java:208)\r\n\tat java.base/java.io.DataInputStream.readInt(DataInputStream.java:385)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:774)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\r\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\r\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat org.apache.spark.ContextAwareIterator.hasNext(ContextAwareIterator.scala:39)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator$GroupedIterator.takeDestructively(Iterator.scala:1160)\r\n\tat scala.collection.Iterator$GroupedIterator.go(Iterator.scala:1176)\r\n\tat scala.collection.Iterator$GroupedIterator.fill(Iterator.scala:1213)\r\n\tat scala.collection.Iterator$GroupedIterator.hasNext(Iterator.scala:1217)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\r\n\tat scala.collection.Iterator.foreach(Iterator.scala:943)\r\n\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\r\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1431)\r\n\tat org.apache.spark.api.python.PythonRDD$.writeIteratorToStream(PythonRDD.scala:322)\r\n\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$PythonUDFWriterThread.writeIteratorToStream(PythonUDFRunner.scala:58)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.$anonfun$run$1(PythonRunner.scala:451)\r\n\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1928)\r\n\tat org.apache.spark.api.python.BasePythonRunner$WriterThread.run(PythonRunner.scala:282)\r\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# Configuración del Entorno y Carga de Bibliotecas\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definición de Funciones Auxiliares\n",
    "def load_target_pages(labels_path, document_name):\n",
    "    with open(labels_path, 'r') as f:\n",
    "        labels = json.load(f)\n",
    "    return labels[document_name][\"target_pages\"]\n",
    "\n",
    "def calculate_metrics(predicted_pages, target_pages):\n",
    "    true_positives = len(set(predicted_pages) & set(target_pages))\n",
    "    false_positives = len(set(predicted_pages) - set(target_pages))\n",
    "    false_negatives = len(set(target_pages) - set(predicted_pages))\n",
    "\n",
    "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
    "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path, target_pages, output_file):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                label = 1 if page_num in target_pages else 0\n",
    "                image_data.append((image_path, label, features))\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"label\", IntegerType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for row in first_pages:\n",
    "            f.write(row[\"path\"] + '\\n')\n",
    "\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "    predicted_pages = [int(row[\"path\"].split('_')[-1].split('.')[0]) for row in first_pages]\n",
    "    precision, recall, f1_score = calculate_metrics(predicted_pages, target_pages)\n",
    "    print(f\"Precisión: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-score: {f1_score:.4f}\")\n",
    "\n",
    "    return predicted_pages, df_scaled\n",
    "\n",
    "def split_pdf(input_pdf_path, output_dir, first_pages):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_pdf_path, 'rb') as input_pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(input_pdf_file)\n",
    "        total_pages = len(pdf_reader.pages)\n",
    "\n",
    "        first_pages.append(total_pages)\n",
    "\n",
    "        for i in range(len(first_pages) - 1):\n",
    "            start_page = first_pages[i]\n",
    "            end_page = first_pages[i + 1]\n",
    "\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            for page_num in range(start_page, end_page):\n",
    "                pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "\n",
    "            output_pdf_path = os.path.join(output_dir, f'segment_{i + 1}.pdf')\n",
    "            with open(output_pdf_path, 'wb') as output_pdf_file:\n",
    "                pdf_writer.write(output_pdf_file)\n",
    "\n",
    "            print(f'Segmento {i + 1} guardado en {output_pdf_path}')\n",
    "\n",
    "# Ejecución del Modelo y Predicción de Primeras Páginas\n",
    "pdf_dir = 'data/pdf_pages'\n",
    "model_path = 'data/models/logistic_regression_model'\n",
    "scaler_path = 'data/processed/scaler_model'\n",
    "labels_path = 'data/labels.json'\n",
    "document_name = \"2-TITULOS-15-DE-NOVIEMBRE-2024\"\n",
    "target_pages = load_target_pages(labels_path, document_name)\n",
    "output_file = 'predicted_first_pages.txt'\n",
    "predicted_pages, df_scaled = predict_first_pages(pdf_dir, model_path, scaler_path, target_pages, output_file)\n",
    "\n",
    "# Corte del PDF Basado en las Predicciones\n",
    "input_pdf_path = 'data/raw/2-TITULOS-15-DE-NOVIEMBRE-2024.pdf'  # Asegúrate de que esta ruta sea correcta y el archivo exista\n",
    "output_dir = 'data/output'\n",
    "split_pdf(input_pdf_path, output_dir, predicted_pages)\n",
    "\n",
    "# Realizar Tuning del Modelo\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"scaled_features\", labelCol=\"label\")\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"rawPrediction\", metricName=\"areaUnderROC\")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.5]) \\\n",
    "    .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0]) \\\n",
    "    .addGrid(lr.maxIter, [10, 20, 50]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(estimator=lr,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=evaluator,\n",
    "                          numFolds=3)\n",
    "\n",
    "final_df = df_scaled.select(\"label\", \"scaled_features\")\n",
    "train_df, test_df = final_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "cvModel = crossval.fit(train_df)\n",
    "\n",
    "predictions = cvModel.transform(test_df)\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
    "\n",
    "best_model = cvModel.bestModel\n",
    "best_model.write().overwrite().save(os.path.join('data/models', \"best_logistic_regression_model\"))\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Biblioteca nativa de Hadoop cargada correctamente.\n",
      "Páginas predichas como primeras páginas:\n",
      "data/PDFTEST\\page_0.png\n",
      "data/PDFTEST\\page_10.png\n",
      "data/PDFTEST\\page_20.png\n",
      "data/PDFTEST\\page_30.png\n",
      "data/PDFTEST\\page_40.png\n",
      "data/PDFTEST\\page_50.png\n",
      "data/PDFTEST\\page_60.png\n",
      "data/PDFTEST\\page_70.png\n",
      "data/PDFTEST\\page_80.png\n",
      "data/PDFTEST\\page_90.png\n",
      "data/PDFTEST\\page_100.png\n",
      "data/PDFTEST\\page_110.png\n",
      "data/PDFTEST\\page_114.png\n",
      "data/PDFTEST\\page_116.png\n",
      "data/PDFTEST\\page_124.png\n",
      "data/PDFTEST\\page_134.png\n",
      "data/PDFTEST\\page_140.png\n",
      "data/PDFTEST\\page_146.png\n",
      "data/PDFTEST\\page_152.png\n",
      "data/PDFTEST\\page_162.png\n",
      "data/PDFTEST\\page_170.png\n",
      "data/PDFTEST\\page_178.png\n",
      "data/PDFTEST\\page_184.png\n",
      "data/PDFTEST\\page_190.png\n",
      "data/PDFTEST\\page_198.png\n",
      "data/PDFTEST\\page_206.png\n",
      "data/PDFTEST\\page_214.png\n",
      "data/PDFTEST\\page_222.png\n",
      "data/PDFTEST\\page_244.png\n",
      "data/PDFTEST\\page_254.png\n",
      "data/PDFTEST\\page_264.png\n",
      "data/PDFTEST\\page_274.png\n",
      "data/PDFTEST\\page_284.png\n",
      "data/PDFTEST\\page_290.png\n",
      "data/PDFTEST\\page_296.png\n",
      "data/PDFTEST\\page_306.png\n",
      "data/PDFTEST\\page_312.png\n",
      "data/PDFTEST\\page_322.png\n",
      "data/PDFTEST\\page_332.png\n",
      "data/PDFTEST\\page_346.png\n",
      "data/PDFTEST\\page_356.png\n",
      "data/PDFTEST\\page_360.png\n",
      "Segmento 1 guardado en data/output1\\segment_1.pdf\n",
      "Segmento 2 guardado en data/output1\\segment_2.pdf\n",
      "Segmento 3 guardado en data/output1\\segment_3.pdf\n",
      "Segmento 4 guardado en data/output1\\segment_4.pdf\n",
      "Segmento 5 guardado en data/output1\\segment_5.pdf\n",
      "Segmento 6 guardado en data/output1\\segment_6.pdf\n",
      "Segmento 7 guardado en data/output1\\segment_7.pdf\n",
      "Segmento 8 guardado en data/output1\\segment_8.pdf\n",
      "Segmento 9 guardado en data/output1\\segment_9.pdf\n",
      "Segmento 10 guardado en data/output1\\segment_10.pdf\n",
      "Segmento 11 guardado en data/output1\\segment_11.pdf\n",
      "Segmento 12 guardado en data/output1\\segment_12.pdf\n",
      "Segmento 13 guardado en data/output1\\segment_13.pdf\n",
      "Segmento 14 guardado en data/output1\\segment_14.pdf\n",
      "Segmento 15 guardado en data/output1\\segment_15.pdf\n",
      "Segmento 16 guardado en data/output1\\segment_16.pdf\n",
      "Segmento 17 guardado en data/output1\\segment_17.pdf\n",
      "Segmento 18 guardado en data/output1\\segment_18.pdf\n",
      "Segmento 19 guardado en data/output1\\segment_19.pdf\n",
      "Segmento 20 guardado en data/output1\\segment_20.pdf\n",
      "Segmento 21 guardado en data/output1\\segment_21.pdf\n",
      "Segmento 22 guardado en data/output1\\segment_22.pdf\n",
      "Segmento 23 guardado en data/output1\\segment_23.pdf\n",
      "Segmento 24 guardado en data/output1\\segment_24.pdf\n",
      "Segmento 25 guardado en data/output1\\segment_25.pdf\n",
      "Segmento 26 guardado en data/output1\\segment_26.pdf\n",
      "Segmento 27 guardado en data/output1\\segment_27.pdf\n",
      "Segmento 28 guardado en data/output1\\segment_28.pdf\n",
      "Segmento 29 guardado en data/output1\\segment_29.pdf\n",
      "Segmento 30 guardado en data/output1\\segment_30.pdf\n",
      "Segmento 31 guardado en data/output1\\segment_31.pdf\n",
      "Segmento 32 guardado en data/output1\\segment_32.pdf\n",
      "Segmento 33 guardado en data/output1\\segment_33.pdf\n",
      "Segmento 34 guardado en data/output1\\segment_34.pdf\n",
      "Segmento 35 guardado en data/output1\\segment_35.pdf\n",
      "Segmento 36 guardado en data/output1\\segment_36.pdf\n",
      "Segmento 37 guardado en data/output1\\segment_37.pdf\n",
      "Segmento 38 guardado en data/output1\\segment_38.pdf\n",
      "Segmento 39 guardado en data/output1\\segment_39.pdf\n",
      "Segmento 40 guardado en data/output1\\segment_40.pdf\n",
      "Segmento 41 guardado en data/output1\\segment_41.pdf\n",
      "Segmento 42 guardado en data/output1\\segment_42.pdf\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import ctypes\n",
    "import cv2\n",
    "import numpy as np\n",
    "import PyPDF2\n",
    "from pdf2image import convert_from_path\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.ml.linalg import Vectors, VectorUDT\n",
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "from pyspark.ml.feature import StandardScalerModel\n",
    "\n",
    "# Configuración del Entorno y Carga de Bibliotecas\n",
    "def check_hadoop_libraries():\n",
    "    hadoop_home = os.environ.get('HADOOP_HOME')\n",
    "    if not hadoop_home:\n",
    "        print(\"Error: HADOOP_HOME no está configurado.\")\n",
    "        return False\n",
    "\n",
    "    winutils_path = os.path.join(hadoop_home, 'bin', 'winutils.exe')\n",
    "    if not os.path.exists(winutils_path):\n",
    "        print(f\"Error: {winutils_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    hadoop_dll_path = os.path.join(hadoop_home, 'bin', 'hadoop.dll')\n",
    "    if not os.path.exists(hadoop_dll_path):\n",
    "        print(f\"Error: {hadoop_dll_path} no existe.\")\n",
    "        return False\n",
    "\n",
    "    try:\n",
    "        ctypes.cdll.LoadLibrary(hadoop_dll_path)\n",
    "        print(\"Biblioteca nativa de Hadoop cargada correctamente.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: No se pudo cargar la biblioteca nativa de Hadoop: {e}\")\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "if not check_hadoop_libraries():\n",
    "    sys.exit(1)\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable\n",
    "os.environ['HADOOP_HOME'] = 'C:\\\\Program Files\\\\winutils'\n",
    "os.environ['JAVA_HOME'] = 'C:\\\\Program Files\\\\Java\\\\jdk-21'\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['HADOOP_HOME'], 'bin')\n",
    "os.environ['PATH'] += os.pathsep + os.path.join(os.environ['JAVA_HOME'], 'bin')\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PDFFirstPagePredictor\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"10\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Djava.library.path=\" + os.path.join(os.environ['HADOOP_HOME'], 'bin')) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Definición de Funciones Auxiliares\n",
    "def convert_pdf_to_images(pdf_path, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    images = convert_from_path(pdf_path)\n",
    "    for i, image in enumerate(images):\n",
    "        image_path = os.path.join(output_dir, f'page_{i}.png')\n",
    "        image.save(image_path, 'PNG')\n",
    "    return len(images)\n",
    "\n",
    "def predict_first_pages(pdf_dir, model_path, scaler_path, output_file):\n",
    "    if not os.path.exists(model_path):\n",
    "        print(f\"Error: El modelo no existe en la ruta {model_path}\")\n",
    "        return\n",
    "    if not os.path.exists(scaler_path):\n",
    "        print(f\"Error: El escalador no existe en la ruta {scaler_path}\")\n",
    "        return\n",
    "\n",
    "    lr_model = LogisticRegressionModel.load(model_path)\n",
    "    scaler_model = StandardScalerModel.load(scaler_path)\n",
    "\n",
    "    image_data = []\n",
    "    for page_num in range(len(os.listdir(pdf_dir))):\n",
    "        image_path = os.path.join(pdf_dir, f'page_{page_num}.png')\n",
    "        if os.path.exists(image_path):\n",
    "            image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "            if image is not None:\n",
    "                resized = cv2.resize(image, (224, 224))\n",
    "                normalized = resized.astype(np.float32) / 255.0\n",
    "                features = normalized.flatten().tolist()\n",
    "                image_data.append((image_path, 0, features))  # Label is not used\n",
    "\n",
    "    schema = StructType([\n",
    "        StructField(\"path\", StringType(), False),\n",
    "        StructField(\"label\", IntegerType(), False),\n",
    "        StructField(\"features\", ArrayType(FloatType()), True)\n",
    "    ])\n",
    "    df = spark.createDataFrame(image_data, schema)\n",
    "\n",
    "    array_to_vector_udf = udf(lambda x: Vectors.dense(x), VectorUDT())\n",
    "    df = df.withColumn(\"features_vector\", array_to_vector_udf(\"features\"))\n",
    "\n",
    "    df_scaled = scaler_model.transform(df)\n",
    "\n",
    "    predictions = lr_model.transform(df_scaled)\n",
    "\n",
    "    first_pages = predictions.filter(col(\"prediction\") == 1).select(\"path\").collect()\n",
    "\n",
    "    with open(output_file, 'w') as f:\n",
    "        for row in first_pages:\n",
    "            f.write(row[\"path\"] + '\\n')\n",
    "\n",
    "    print(\"Páginas predichas como primeras páginas:\")\n",
    "    for row in first_pages:\n",
    "        print(row[\"path\"])\n",
    "\n",
    "    predicted_pages = [int(row[\"path\"].split('_')[-1].split('.')[0]) for row in first_pages]\n",
    "    return predicted_pages\n",
    "\n",
    "def split_pdf(input_pdf_path, output_dir, first_pages):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    with open(input_pdf_path, 'rb') as input_pdf_file:\n",
    "        pdf_reader = PyPDF2.PdfReader(input_pdf_file)\n",
    "        total_pages = len(pdf_reader.pages)\n",
    "\n",
    "        first_pages.append(total_pages)\n",
    "\n",
    "        for i in range(len(first_pages) - 1):\n",
    "            start_page = first_pages[i]\n",
    "            end_page = first_pages[i + 1]\n",
    "\n",
    "            pdf_writer = PyPDF2.PdfWriter()\n",
    "            for page_num in range(start_page, end_page):\n",
    "                pdf_writer.add_page(pdf_reader.pages[page_num])\n",
    "\n",
    "            output_pdf_path = os.path.join(output_dir, f'segment_{i + 1}.pdf')\n",
    "            with open(output_pdf_path, 'wb') as output_pdf_file:\n",
    "                pdf_writer.write(output_pdf_file)\n",
    "\n",
    "            print(f'Segmento {i + 1} guardado en {output_pdf_path}')\n",
    "\n",
    "# Rutas de Entrada y Salida\n",
    "pdf_path = 'data/raw/2-TITULOS-15-DE-NOVIEMBRE-2024.pdf'  # Ruta del archivo PDF de entrada\n",
    "images_output_dir = 'data/PDFTEST'  # Directorio donde se guardarán las imágenes\n",
    "model_path = 'data/models/logistic_regression_model'  # Ruta del modelo\n",
    "scaler_path = 'data/processed/scaler_model'  # Ruta del escalador\n",
    "output_file = 'predicted_first_pages2.txt'  # Archivo donde se guardarán las predicciones\n",
    "output_dir = 'data/output1'  # Directorio donde se guardarán los PDFs cortados\n",
    "\n",
    "# Convertir el PDF en Imágenes\n",
    "num_pages = convert_pdf_to_images(pdf_path, images_output_dir)\n",
    "\n",
    "# Hacer la Predicción de las Primeras Páginas\n",
    "predicted_pages = predict_first_pages(images_output_dir, model_path, scaler_path, output_file)\n",
    "\n",
    "# Cortar el PDF Basado en las Predicciones\n",
    "split_pdf(pdf_path, output_dir, predicted_pages)\n",
    "\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
